---
title: "Statistical modelling Part 2"
output: 
    learnr::tutorial:
      theme: readable
      progressive: true
      allow_skip: false
runtime: shiny_prerendered
subtitle: "2022"
description: "2022 BYU22S01 Practical"
---


```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE, comment = NA)
crabs <- MASS::crabs
crabs_lm <- lm(CL ~ CW, crabs)
crabs_add <- lm(CL ~ CW + sp, crabs)
crabs_int <- lm(CL ~ CW * sp, crabs)
palette(c("#0072B2", "#D55E00"))
tutorial_options(exercise.completion = FALSE)
test_data <- data.frame(concentration = sample(size = 10, 1:20))
```

## How to use this worksheet

This worksheet is an interactive document run through RStudio. It is made using the R package `learnr`, which you installed before the prac. The worksheet has interactive code chunks that you can use to enter `R` code and quizzes to test yourself - which makes this more fun than a regular word document or PDF and lets you practice R coding.

You can interact with this worksheet through the Tutorial tab in RStudio. This window can be resized and expanded. However, I recommend clicking the pop out button (in between the home icon and the stop icon) to open the tutorial in a separate window. 

***

**Code chunks**  
Code chunks are independent of the main RStudio Environment. Anything you type here will not be saved to memory, but you won't need to. Try it:  
*Here's a simple exercise with an empty code chunk provided for entering the answer. Click __Run Code__ to run the code. The __Hint__ button will tell you a hint. If you are really stuck, the last hint may be the solution.*

Write the R code required to divide 10 by 5 (the answer should be 2):

```{r test, exercise=TRUE, exercise.lines = 5}

```

```{r test-hint}
You need a / somewhere. You do not need =
```

```{r test-solution}
10/5
```

You can use RStudio and R scripts at the same time for trying things out or taking notes. 

***

**Quizzes**  
**Quizzes here are not part of the CA.** The questions are meant to check you understand the material and your answers are not visible to anyone else. The CA is accessed separately.

*Here's a quiz. Try it out to see how it works. Click __submit__ and it will tell you what you got correct. What happens when you put an incorrect answer in?*
```{r test-quiz}
quiz(caption = "Answer the following questions:",
question(
  "What colour is a grey squirrel?",
  answer("Red"),
  answer("Brown"),
  answer("Grey", correct = TRUE),
  answer("Black"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question_text(
  paste("What is the answer to the above coding exercise?"),
  answer("2", correct = TRUE),
  allow_retry = TRUE
),
question_checkbox(
  "Which of these are fish? Select all that apply.",
  answer("Seadragon", correct = TRUE),
  answer("Starfish"),
  answer("Jellyfish"),
  answer("Cuttlefish"),
  answer("Alligator gar", correct = TRUE),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
)
)
```

***

Well done!  
Your answers are saved if you open the tutorial again. You can reset your answers by clicking **Start Over** at the end of the menu. You can run these tutorials any time you like if you want to practice again. 

Although you can come back to this tutorial at any time, I recommend taking your own notes for your reference. There are also discussion questions in bold that you should think about or write down an answer.  

Click **Next Topic** to progress to the practical instructions.  
You can navigate between topics using the buttons or the menu.  

> You must complete all the exercises to move ahead.  
> Do not skip exercises - This is the "practice" of the practical

***

## Introduction

Welcome to Part 2! We will continue to look at statistical modelling using the data we collected in the previous practical. 

In this practical we aim to take a practical and hands-on approach to learning statistical modelling by using R and a real dataset you collected. We focus on the practicalities of *applying* statistics (implementation & interpretation) - not on the theory or mathematics (which was covered earlier in semester).

This practical is split into three parts (recommended time to spend on each section):

Part A: Using additive multiple linear regressions (models) in R (60 mins)  

We will skip simple linear regressions because the same concepts and process apply to simple regressions as with multiple regressions.

Part B: Using multiplicative multiple linear regressions (models) in R (60 mins)  

Parts A and B have the same learning objectives.  
**Learning objectives:**

 * Know how to run a linear regression in R with biological data
 * Know how to parametrise a linear regression from R output 
 * Know how to use linear models to predict new values
 * Know how to interpret linear regression output to test hypotheses

> We have covered all of these concepts in the lectures. This prac will reinforce and revise these concepts and allow you to practise them under supervision from the demonstrators.

Part C: Visualising data (30 mins)  
**Learning objectives:**

 * Know how to make a graph in R with appropriate labels
 * Know what to include in a figure caption

This was also covered earlier in semester in the practicals and in lectures.

Each part has two sections:

1. A theory section where we will recap the lecture content and go over the practicalities of linear regression in R step by step using an example dataset. There are code chunks and revision questions. You can retry questions you get wrong - learning is about the process not the marks!
2. A practice section where you will apply your new skills to the data you collected last practical. This is required to answer the CA for Parts A & B. You will need access to the class dataset from the previous practical so save it somewhere you know.  

**I do not recommend skipping the theory before attempting the CA - the theory section will go through all the steps you need for analysing your data**. The demonstrators aren't allowed to help you answer the CA directly but they can help you understand the theory parts so that you can answer the CA. There are no CA questions associated with Part C but that doesn't mean you should skip it because it allows you to practise.

> Don't forget to answer the CA questions in a separate link. I recommend answering the CA as you progress along the prac when prompted but don't submit until the end. You could also wait and do the CA in one go at the end.

The CA aims to assess your understanding of the concepts in this practical and your ability to apply the concepts in practice to new scenarios.

***

## How to problem solve
### Some general advice for programming and learning

We don't expect you to be expert programmers but we *do* expect you to be able and willing to figure things out yourself (problem solving).

In programming, it's *really easy* to make mistakes that breaks code. That is no means a reflection on you or your ability to code/learn.  

If your code is not working take a moment to *breathe*. Then check for common, minor errors such as:

 * Spelling mistakes
 * Wrong dataset name
 * Wrong variable (column) name
 * Missing or wrong quotation mark
 * Missing bracket
 * Inconsistent cases (e.g. Uppercase)
 * Missed a step
 * Invalid syntax (e.g. spaces)
 * Duplicates of the same function with multiple errors - keep your scripts tidy!

These reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.
 
 > You can **and should** easily fix the above mistakes yourself!

The point of learning to code is to not treat statistical software like a black box where you cannot see inside and you don't know where the numbers came from.

**Do not take the code here for granted. One does not simply copy code from somewhere and blindly expect it to work out of the box.**
 
Programming is like learning the grammar and syntax of another language and writing a script is like writing a recipe for a computer. Each line of code has a purpose and meaning. Computers only do what they are told, the rest is up to you.

*** 

### Talk to a rubber duck

You need a clear idea of what you want to achieve in programming. Articulate in words to yourself what you aim to do, what you expect to see if it worked, what you did leading up to this point and what you think is happening to cause the error. This is called the rubber duck method - like you are explaining your problems to a rubber duck.

If there's an error message, read it. Error messages are the computer telling you what's wrong. Try googling the entire error message to see what other people have said. The entire error message is necessary, not just the last bit. 

> There's no shame in googling everying - that's what we do too and part of learning to work independently

Take the time to *synthesise* and *integrate* information so that you can apply it to *new scenarios* - that is the learning objective of your entire university degree. And what we will be doing here.

Properly read and digest the material you are given because almost all your assessments will test your reading comprehension and attention to detail. And there's nothing in this practical that wasn't covered earlier in semester and in the lectures.

Don't skim read. Take notes. Follow the written instructions.

 > Learning is about the process and about your growth, not the end result or your grade

One of the hard things about troubleshooting someone else's programming is that it's not clear what the problem is. When describing your problem, be comprehensive & give context. Show the entire error message, not just parts of it. Trace your steps leading up to it. 

The more information you provide, the more information and context people have to help you. That's why we might ask you to provide more information or ask further questions to troubleshoot. The first things we would check are the steps above (minor human errors), so if you've done those, then say what you've already checked and what causes you've ruled out. That gives us a starting point to assist you further.

Programming is **trial and error**. You shouldn't expect to get it on the first try. And that's OK - it's part of the process.

***

###

 > We **really** advise you to take your time, work through the material in order and practice before you even look at the assessment. Refer to your lecture notes or previous practicals.   
 
 > Unless you are 100% confident with data handling and doing multiple regressions in R, do not skip ahead to answer your assessment on the fly.
 
 > There is nothing here that you don't need to know. 
 You have been warned!
 
 
Test your problem solving.  

*The following code should calculate the median of a column called `concentration` in a dataset called `test_data` but it doesn't work.*  
`median(concentration$test_data)`
```{r reading}
question(
  "Why doesn't the above code work?",
  answer("Spelling mistake"),
  answer("Wrong function structure", correct = TRUE, message = random_praise()),
  answer("Wrong function to calculate medians"),
  answer("Missing bracket"),
  answer("Missing quotation marks"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
)
```

*Enter in the correct code to calculate the median of `concentration`. Press Run Code.*
```{r reading-exercise, exercise = TRUE}

```

```{r reading-ans}
ans_conc <- median(test_data$concentration)
question_text("What is the median of concentration?",
  answer(paste(ans_conc), correct = TRUE, message = random_praise()),
  allow_retry = TRUE,
  incorrect = random_encouragement()
)
```

***

## Multiple linear regressions

We saw the statistical theory of linear regressions in the lectures. We will use the dataset `crabs` - see `help(crabs)` for more information. This dataset is provided in `R` in the package `MASS`. I have already loaded the data for you within the tutorial so you don't need to do it but you will need to load it via `library(MASS)` if you want to try code in a script. What happens in the interactive tutorial is independent of R's Environment.

There are three variables we will look at (carapace is the zoological term for shell):

 * `CL`: Carapace length (mm) - our response variable
 * `CW`: Carapace width (mm) - our predictor variable
 * `sp`: Colour morph (`B` or `O` for blue or orange crabs)

The code we use here can be applied to any dataset. You can try it with any biological system of your choice in your own time. For example, `airquality` has environmental data or the Penguins [dataset](https://towardsdatascience.com/penguins-dataset-overview-iris-alternative-9453bb8c8d95) about penguins can be downloaded as a package.
 
We can ask the question "Does the relationship between shell length and width differ between colour morphs?". We can also phrase this as "Does the relationship between shell length and width depend on the colour of the crab?". Compared to a simple linear regression with one predictor variable (e.g. `CW`), this is a more complex question needing a more complex experimental design (two predictor variables) and thus a more complex statistical analysis.

> Multiple regression allows us to ask "Does including information about our predictor variables improve our ability to detect/understand trends in our response variable?"

It will help to see our data graphically.
```{r crab-graph, fig.cap= "Do you think the relationship between shell length and width differs with the colour of the crab?"}
plot(CL ~ CW, crabs, pch = 20, col = sp, bty = "l")
legend("bottomright", bty = "n", c("Blue", "Orange"), pch = c(20, 20), col = c(1,2))
```

There are two general types of multiple regression model:

1. Additive model (**Fixed slopes model**)
2. Interactive model (**Random slopes, full or multiplicative model**)

The difference is in how they calculate the slope and the intercept of the linear regression. **This changes the interpretation of the model and the associated hypotheses.**

```{r quiz1}
question("What is the difference between a simple linear regression and a multiple linear regression?",
         answer("There is no difference"),
         answer("Simple linear regressions have two predictor variables, multiple linear regressions have one"),
         answer("Simple linear regressions have one predictor variable, multiple linear regressions have two or more", correct = TRUE, message = random_praise()),
         answer("Simple linear regressions have two response variables, multiple linear regressions have one"),
         answer("Simple linear regressions have one response variable, multiple linear regressions have two or more"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

We need to fit a linear model to this data. Remember linear models with two predictor variables *and* an interaction (a **full** interactive model) take the general form:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$$

Where $X_1$ is our first predictor variable and $X_2$ is our second predictor variable. $\varepsilon$ is the random error (residual error) not accounted for by the model - it's always part of the equation but it's often ignored, much like $c$ in integration.

```{r slope}
quiz(caption = "Answer the following questions:",
  question(
  "What does $\\beta_1$ represent?",
  answer("response variable"),
  answer("slope", correct = TRUE, message = random_praise()),
  answer("predictor variable"),
  answer("intercept"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
),
  question(
  "What does $\\beta_0$ represent?",
  answer("response variable"),
  answer("intercept", correct = TRUE, message = random_praise()),
  answer("predictor variable"),
  answer("slope"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
))
```

We need to calculate the values of $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$ to **parameterise** the model. We could do it by hand for each observation, but that becomes an impossible task for large datasets. That's where R comes in.

## Part A: Additive models

The additive linear regression takes the general form:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon$$

> In additive models the effect of one predictor on the response variable is **additive** of the other.

There is **no interaction** between the predictor variables colour or shell width (i.e. $\beta_3 = 0$). Thus the two fitted lines have the same slope ($\beta_1$) - hence they are sometimes called *fixed slopes* models.

To find these coefficients, the ordinary least squares regression splits the dataset into its predictor variables and fit a model to each component - this is called **partial regression**. The goal is to assign as much variation in the data as possible to each predictor variable. This process requires a baseline variable that sets the contrast for assigning variation.

Dummy variables are used with the categorical predictor to set the baseline for the ordinary least squares regression. Since Blue is alphabetically before Orange, Blue is the baseline contrast used in the partial regression process and assigned the dummy variable 0. 

Thus for a blue crab: $CL = \beta_0 + \beta_1 CW + \beta_2 sp \times 0 + \varepsilon$ becomes $CL = \beta_0 + \beta_1 CW$  
An orange crab gets a dummy variable of 1, thus: $CL = \beta_0 + \beta_1 CW + \beta_2 sp \times 1 + \varepsilon$ becomes $CL = (\beta_0 + \beta_2) + \beta_1 CW$   

In effect, we are fitting *two* lines to this data - one for each level of colour. The technical term for each of these lines is a **partial regression line**. We can write this as:

$$ shell \space length  = \beta_{0_{colour}} + \beta_{1} shell \space width + \varepsilon$$
Now the intercept parameter ($\beta_{0_{colour}}$) specifies that it is dependent on the colour of the crab: $\beta_{0_{colour}} = \beta_0 + \beta_2$.

Our hypotheses are:

* H0: There is no effect of shell width or colour on shell length
* H1: There is an effect of shell width or colour on shell length

***

### Additive models in R

A linear regression in R with two predictor variables follows the general formula: 
```
lm(Y ~ X1 + X2, data)
```
Where:

 * `X1` & `X2` are the two predictor variables
 * `+` indicates the relationship between the two predictors: a plus sign for an *additive* relationship
 * `lm` stands for linear model
 * `Y` is our response variable
 * `data` is the name of our dataset 
 * `~` indicates a relationship between our response and predictor variables

In `crabs`, our response variable is `CL` and our predictor variables are `CW` and `sp`. 

*The function below does an additive multiple regression in R but there is one mistake. Fix the error and run the code.*
```{r fixed-mod, exercise = TRUE}
lm(CL ~ CW + sp)
```
```{r fixed-mod-hint}
Are all components of the lm function included?
```

Did you get some output ?  
It should tell us two things:

1. Call is the formula used. It should be the same as the linear model code
2. Coefficients are the estimated coefficients of the model. From left to right they are: $\beta_0$, $\beta_1$ and $\beta_2$.

Let's start with the first two coefficients. The interpretation for these coefficients is the same as simple linear regression. There's the intercept `(Intercept)` ($\beta_0$) and there's the slope `CW` ($\beta_1$).

**But** remember we are expecting *two* lines in our model - one for blue crabs and one for orange crabs. **Which line are these first two coefficients referring to?**

Here are the coefficients $\beta_0$, $\beta_1$ and $\beta_2$:
```{r coef-fixed}
coef(crabs_add)
```


```{r coef-cals}
quiz(
  caption = "Answer the following questions:",
  question(
    "How are these coefficients calculated?",
    answer("Ordinary least squares regression", correct = TRUE, message = random_praise()),
    answer("Ordinary most squares regression"),
    answer("Random regression"),
    answer("Random squared regression"),
    incorrect = "Incorrect. Remember back to the lecture on how linear regressions are parameterised",
    allow_retry = TRUE,
    random_answer_order = TRUE
  ),
  question_radio(
    "Given what you know about how R orders levels in a factor in alphabetical order, for which level do you think the first two coefficients are modelling?",
    answer("Blue crabs", correct = TRUE, message = "B comes before O so the first two coefficients are the intercept and slope of the linear regression for blue crabs. It also says spO in the next coefficient which tells you that is for orange crabs"),
    answer("Orange crabs"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question(
    "The value for the slope is positive. What does that mean?",
    answer("As crab shell width increases, crab shell length decreases"),
    answer(
      "As crab shell width increases, crab shell length increases",
      correct = TRUE, message = random_praise()
    ),
    answer("As crab shell width increases, crab shell length stays the same"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```


### Parameterising our model

So, we already have our equation for blue crabs with $\beta_0$ and $\beta_1$.  
Parametrised equation for blue crabs:  
*blue crab shell length* = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` *shell width*

Halfway there! Now for the orange crabs!

Since the slope of the regression for orange crabs is the same as blue crabs, we already know the value of $\beta_{1}$ is `r round(coef(crabs_add)[2],1)`. But we need to manually calculate the intercept for orange crabs.

Like in an ANOVA, the estimate for `spO` is $\beta_2$, which is the **difference in the intercept between orange crabs and blue crabs**. To calculate the intercept for orange crabs we need to add the estimated coefficient of the intercept for blue crabs ($\beta_0$) with the difference ($\beta_2$). 

Now that you know how to parameterise the linear regression for orange crabs:  
```{r showaddcoef}
coef(crabs_add)  
```

```{r param, echo=FALSE}
coef <- round(crabs_add$coefficients, 1)
question("What is the paramterised model for orange crabs? To 1 decimal place",
         answer(paste0("CL = ", (coef[1]+coef[3]), " + ", coef[2], "CW"), correct = TRUE, message = random_praise()),
         answer(paste0("CL = ", coef[1], " - ", coef[2], "CW"), message = "Incorrect. This is the regression for blue crabs"),
         answer(paste0("CL = ", coef[3], " + ", coef[2], "CW"), message = "Incorrect. This is the difference in the intercept"),
         answer(paste0("CL = ", coef[1], " + ", coef[3], "CW"), message = "Incorrect. This is the difference in the intercept"),
         allow_retry = TRUE,
         random_answer_order = TRUE
)
```

> R shows the difference between parameter estimates so you need to calculate the correct values. They are the partial regression coefficients that show the change in the response variable with one predictor variable **while holding all other predictor variables constant.**

For example, using the mean value of orange crabs to estimate coefficients of blue crabs and vice versa, because the mean value of each of these groups represents the null hypothesis. 

In other words, if we were to accept the null hypothesis that there is no relationship between shell width and colour on shell length, then the slope of the line should be 0 and the intercept of the line should be the sample mean of shell length (ignoring shell width and colours).

***

### Practice: Parameterising a linear model of predator-prey functional response

Remember we had some unknown parameters in our predator-prey functional response model? The search rate, $a$, and the handling time, $T_h$. We need to find the value of those parameters from the slope and the intercept of our linearised Type II dataset - exactly like we did for the `crabs` example. We can use the `lm` function on our predator-prey model data to parameterise our model.

You can remind yourself of how the Type II functional response model is derived from the documentation file - run `vignette("TypeII_models")` in Console.

Before we can do our `lm` in `R` we need to import the data and linearise the data.

**Step 1: Importing data**  
First, we need to import the data in to R. This should be familiar to you from before. 

> * Don't use the code chunks in this tutorial to import data, it won't work. Make your own script (File -> New Script, or Ctrl(Cmd) + Shift + N). 
> * Don't write your code directly in the console either - you won't have a good record of what you've done (in case you wanted to use the code here to help you with your final report). 
> * You can copy the code in this tutorial and **modify as appropriate for your data**. 

The examples here use mathematical notation as the variable names here but the class dataset uses the names from Part 1. You should know which notation matches with what variable description.

The class dataset is provided as a comma separated values file (`.csv`). The function to import a csv file is `read.csv`:

```
class_data <- read.csv("directory/folder/class_data.csv")
```
This imports the spreadsheet into an R object called `class_data` but you can use whatever name you want. You need to replace the file address within the quotation marks with where ever you saved your file on your computer. File housekeeping is important!

> Refer to the previous drop in sessions and practicals to revise these basic steps. Make sure your address is correct and don't forget .csv at the end.

Remember we have three variables of interest in our data:

 * `prey_captured`: The response variable
 * `prey_density`: The first predictor variable
 * `foraging_strategy`: The second predictor variable

The general function to plot a graph in `R` is `plot(Y ~ X, data)` replacing `X`, `Y` and `data` with the respective information from your dataset. By default, the graph will plot individual points.

**Plot your graph of prey captured against prey density.**  
Does your data match what you'd expect from a Type II functional response? 

```{r funct_resp, echo=FALSE, fig.cap= "Type II functional response of an predator-prey response"}
prey <- seq(0, 60, 5)
type2 <- (prey*0.7)/(1+(prey*0.7*0.15))
plot(type2~ prey,
     type = 'l',
     lwd = 4,
     xlab = "Prey density", ylab = "Number of prey captured",
     xaxt='n', yaxt='n')
```

**Step 2: Linearising the data**  

Remember our type II model takes the equation:

$$H_a=\ \frac{a\times H\times T_{total}}{1+a\times H\times T_h}$$

The model is not linear.

```{r linear, echo=FALSE}
question("Why is the Type II model not linear?",
         answer("It has parameters that are divided by another parameter", correct = TRUE, message = random_praise()),
         answer("It has parameters that aren't exponents"),
         answer("It does not have a slope parameter"),
         answer("It does not have an intercept parameter"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

Non-linear methods of parametrising these equations are beyond the scope of this module. But we can make this equation linear and use linear regression to get an estimate for the unknown parameters (see `vignette("TypeII_models")` to refresh your memory).

We need to linearise the data to match a linearised type II model: 

$$\frac{1}{H_a}=\ \frac{1}{a}\times\frac{1}{H\times T_{total}}+\frac{T_h}{T_{total}}$$
The graph should look like this:
```{r tyepe2line, echo = FALSE, fig.cap= "Linearised type II functional response of an predator-prey response"}
prey <- seq(0, 0.25, 0.01)
type2 <- seq(0, 1, length.out = length(prey))
plot(type2 ~ prey , type = 'l', lwd = 4, ylab = "1/Ha, Prey captured", xlab = "1/H Total time, Prey density",
     xaxt='n', yaxt='n')
```


```{r recap, echo = FALSE}
quiz(caption = "In case you forgot",
     question("What is does $H_a$ represent?",
              answer("Prey density"),
              answer("Prey eaten", correct = TRUE),
              answer("Number of predators"),
              answer("Duration of the predator-prey interaction"),
              allow_retry = TRUE),
     question("What is does $H$ represent?",
              answer("Prey density", correct = TRUE),
              answer("Prey eaten"),
              answer("Number of predators"),
              answer("Duration of the predator-prey interaction"),
              allow_retry = TRUE),
     question("What is does $T_h$ represent?",
              answer("Prey density"),
              answer("Prey eaten"),
              answer("Number of predators"),
              answer("Time spent to capture one prey", correct = TRUE),
              allow_retry = TRUE),
          question("What is does $T_{total}$ represent?",
              answer("Prey density"),
              answer("Time spent to handle one prey"),
              answer("Time spent between each prey item"),
              answer("Total time spent foraging", correct = TRUE),
              allow_retry = TRUE))
```

Let's first take the inverse of our response variable $H_a$ to get $\frac{1}{H_a}$

```
class_data$Ha.1 <- 1/class_data$Ha
```
This line of code calculates the inverse of `Ha`, that is, 1 divided (`/`) by the number of prey captured (`Ha`), then saves that number to a new column in our dataset called `Ha.1`. Note the use of no spaces.

When manipulating data like this, it's best practice to add new columns to the data, rather than overwrite the original column. That way, if you make a mistake it's easier to see what went wrong and you won't have to start from the beginning!

> `data$column` is the general structure to select a column in R. Make sure that your dataset name and your column names in your code matches your data name and columns, **It may not exactly match what is written here**.

Now let's linearise the predictor variable (`H`) to get $\frac{1}{H\times T_{total}}$. 

```
class_data$HT.1 <- 1/(class_data$H * class_data$T_total)
```
Because we use a value of 1 minute for $T_{total}$, we are essentially dividing 1 by only prey density ($H$). Told you a value of 1 would make our maths easier!

Since we have done some division, it's a good time to check for any undefined values in case we divided by 0. In `R` this is denoted as infinities (`Inf`). You can check how many infinities there are using `table(is.infinite(class_data$Ha.1))` - this will check whether each cell has undefined values (`is.infinite`) and give you logical `TRUE`/`FALSE` output, then tabulate the logic statements to count the number of `TRUE`/`FALSE` occurrences in the column `Ha.1`.

We can replace the infinities with zeroes using `class_data$Ha.1 <- ifelse(class_data$Ha.1 == Inf, 0, class_data$Ha.1)`. You might recognise the if else statement and understand what's happening from previous lectures: **if** there is an undefined value, **then** replace that value with 0, **else** leave the value as is. We don't discard these observations because a value of 0 has biological meaning.  

We now have all the correct columns to parameterise our functional response.

> Make sure that your column names and the name of your dataset in your code matches the names you are using. They may not be exactly the same but the structure of the function is the same (`data$column`). Your column names can be whatever is meaningful to you. E.g. you don't *have* to call your new column `HT.1` but recognise that you need to remember to modify your code accordingly.

**Step 3: Doing the linear regression**  
If `lm` is the function to do a linear regression, `Ha.1` is the name of our response variable, `HT.1` is the name of the first predictor variable, `foraging_strategy` is the name of the second predictor variable, and the name of our dataset is `class_data`, you should be able to write the code for the linearised type II functional response (or replacing the respective components with the column name and dataset names you are using). 

(Hint: see the theory section above if stuck)

```{r error, echo = FALSE}
quiz(caption = "Getting an error message?",
     question("What gets recorded if the predator did not catch any prey?",
              answer("Ha = 0", correct = TRUE),
              answer("Nothing"),
              answer("Ha > 0"),
              answer("Ha < 0"),
              allow_retry = TRUE,
              random_answer_order = TRUE),
     question("What happens in R when you divide by 0?",
              answer("The value is undefined (Inf)", correct = TRUE),
              answer("It works"),
              answer("R crashes"),
              answer("It is the wrong function"),
              allow_retry = TRUE,
              random_answer_order = TRUE),
     question("What is the solution?",
              answer("Change the values to 0", correct = TRUE),
              answer("Delete the data"),
              answer("Collect new data"),
              answer("Use a different program"),
              allow_retry = TRUE,
              random_answer_order = TRUE,
              incorrect = "We don't throw out valid data"))
```

**Construct a linear model for our data.**  
Then, you should be able to identify the values for the slope and intercept and thus parameterise the linearised function in Step 4 - exactly as you did with the `crabs` dataset.

> Linear regressions are always done on the entire data, not on averages. i.e. you wouldn't use the average of your replicates for each treatment for the underlying data. To fit a line to data, ordinary least squares regression depends on quantifying *variation* of observations around the mean (think back to how sampled data from a population is distributed). Averaging data removes that variation and thus there is less information for `R` to use (fewer degrees of freedom).

We won't plot our regression lines just yet but you should be able to interpret the R output as a graph by looking at the coefficient values. We will construct a graph in Part C of the practical!

**Step 4: Parameterising $a$ and $T_h$**  
The final step is to get our unknown values of $a$ and $T_h$ from our linear regression. Since we know our regression takes the form $Y  = \beta_0 + \beta_1 X_1$ and we know the linearised type II model is $\frac{1}{H_a}=\ \frac{1}{a}\times\frac{1}{H\times T_{total}}+\frac{T_h}{T_{total}}$, then you should have all the information to parameterise the type II model and derive values for $a$ and $T_h$ from the linear regression output for both foraging strategies.

> Answer the CA

***

### Predicting new values

> One application of statistical models is to make predictions about outcomes under new conditions

We can calculate the value of the response variable from any given value of the predictor variable. You need a fully parameterised model to do this - which we have! 

***

For example, we can use the parametrised equation of a simple linear regression (ignoring colour), $CL =$ `r round(coef(crabs_lm)[1],1)`  + `r round(coef(crabs_lm)[2],1)` $\times CW$, to work out the length of a crab for any value of shell width. 

If a crab is 10 mm wide, what is its predicted shell length?

* We are told the value of shell width (10 mm)
* We know the parameterised linear model:

$CL =$ `r round(coef(crabs_lm)[1],1)`  + `r round(coef(crabs_lm)[2],1)` $\times CW$

* We can substitute the value of 10 for CW into our parameterised model:

$CL =$ `r round(coef(crabs_lm)[1],1)`  + `r round(coef(crabs_lm)[2],1)` $\times 10$  

* and solve for length:  
$CL =$ `r round(coef(crabs_lm)[1],1) + 10 * round(coef(crabs_lm)[2],1)` mm

***

Now, we can use our two partial regression models to predict the shell length of blue or orange crabs.  

*Blue crab shell length* = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` *shell width*  
*Orange crab shell length* = `r round(coef(crabs_add)[1] + coef(crabs_add)[3],1)` + `r round(coef(crabs_add)[2],1)` *shell width*

```{r predict-fixed}
ccoef <- round(coef(crabs_add),1) 
ccoef_orange <- ccoef[1] + ccoef[3]

blue <- sample(10:70,1)
ans_blue <- round(ccoef[1] + (ccoef[2] * blue), 1)

orange <- sample(10:70,1)
ans_orange <- round(ccoef_orange + (ccoef[2] * orange), 1)

size <- sample(10:70,1)
###

quiz(caption = "Use the equation above to answer the following questions:",
  question_text(
    paste("What is the carapace length of an orange crab with a carapace width of", orange, " mm? To 1 decimal place"),
    answer(paste(ans_orange), correct = TRUE, message = random_praise()),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_text(
    paste("What is the carapace length of a blue crab with a carapace width of", blue, " mm? To 1 decimal place"),
    answer(paste(ans_blue), correct = TRUE, message = random_praise()),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_text(
    paste("How much longer is an orange crab than a blue crab with a shell width of ", size, " mm? To 1 decimal place"),
    answer(paste(round(ccoef[3],1)), correct = TRUE,
           message = "Did you do the whole calculation or notice the answer is the spO coefficient? The slopes are parallel so the answer is the difference in intercepts! The answer is the same regardless of size."),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

***

### Evaluating hypotheses

> Remember that statistical models may represent hypotheses

Remember our hypotheses are:

* H0: There is no effect of shell width or colour on the response variable
* H1: There is an effect of shell width or colour on the response variable

We see from our predicted lines (below) that if the slope ($\beta_1$) of the line is 0 (red), then we accept the null hypothesis and reject the alternative hypothesis. If the slope is different to 0 (blue), then we reject the null hypothesis and accept the alternative.

```{r treehypo, fig.cap="Our predicted linear regressions for the null (red) and alternative (blue) hypotheses"}
plot(CL ~ CW, crabs, pch = 16)
abline(crabs_lm, lwd = 4, col = "blue")
abline(mean(crabs$CL), 0, lwd = 4, col = "red")
```

Note that in our alternative hypothesis we *did not* specify the direction of the relationship (positive or negative), thus we would accept either a positive or negative slope as support for H1. We could be more specific when formulating hypotheses or formulate more than one alternative hypothesis (e.g. H2, H3 etc). 

**So we need to calculate the slope of the line. How do we do that?**  

We've already done it with our `lm` code.  
*Run the additive linear model again to get the slope estimate*
```{r hypotheses-lm, exercise=TRUE, exercise.lines = 5}

```

We get a slope of `r round(coef(crabs_add)[2],1)`. **But is this enough evidence for us to accept/reject hypotheses?** No. What if the slope is not 0 because of random chance? We need to be confident that our estimated slope is *significantly different* to 0. How do we do that?

> Side note: Statistical significance is not the same thing as biological significance. A relationship between two purely randomly generated numbers can be statstically significant but have no biological meaning! Language matters when presenting results.

The predicted slope of the line for the null hypothesis can be considered a *known* population level value.  
Our observed slope of the line from empirical data can be considered an estimated/observed population value.  
We need a statistical test of comparing an observed population value to the known population value.   
**Do you know of one such test from previous lectures?**

```{r tests}
question("Which of these statistical tests you've already learnt in this module would test whether our observed slope is significantly different to 0?",
                  answer("One sample t test", correct = TRUE, message = random_praise()),
                  answer("Analysis of Variance"),
                  answer("Two sample t test"),
                  answer("Paired t test"),
                  allow_retry = TRUE,
                  random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

`R` has already done this automatically as part of `lm` but this additional information is hidden from us.  

To see more information about our linear regression we need to ask to see the `summary` of our linear regression by placing our `lm` function within `summary`.

*Enter the code to check the `summary()` of the additive linear regression for the `crabs` dataset:*
```{r fixed-summary, exercise = TRUE}

```
```{r fixed-summary-hint-1}
Check the summary of the model with summary(). What goes inside summary()?
```
```{r fixed-summary-hint-2}
What goes inside lm()?
```
```{r fixed-summary-hint-3}
Have you included both colour (sp) and shell width (CW) to your model formula?
```

When you run `summary` you get a lot of information. Let's break it down from top to bottom:

* Call is the formula used to do the regression
* Residuals are the residuals of the ordinary least squares regression
* Coefficients are the estimated coefficients we saw earlier *plus* the standard error of these estimates, a t-value from a **one sample t-test** testing whether the estimated coefficient is significantly different to 0 and the P value of this t-test
* Some additional information about the regression at the bottom which we won't look at now

In `summary()` we see `R` has done a series of t-tests on the estimated coefficients. As in a simple linear regression the null hypothesis tested on the intercept and slope for blue crabs is whether these estimates are different to 0. This null hypothesis is not very informative for the intercept - it is more informative about the slope because it tests our overall H0.

We expect only one slope coefficient, `CW`, because our lines should have the same slope. Look at the P-value column of the slope (`CW`) in the `summary` above.

```{r fixed-test}
quiz(caption = "Answer the following questions:",
  question_radio(
    "Based on the t-test on the slope coefficient above, would you accept or reject the null hypothesis?",
    answer("Accept"),
    answer("Reject", correct = TRUE, message = "The P value is less than 0.05"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_radio(
    "Which statement matches your conclusion?",
    answer("There is no effect of shell width or colour on shell length"),
    answer("There is an effect of shell width or colour on shell length", correct = TRUE, message = random_praise()),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

***

### Putting results into sentences

 > We must always place our statsitical analysis into the wider context of our hypotheses and aims. 
 
Communicating our results is important. 

Statistical analyses need to be written into sentences, within a results section. Screenshots of R output are not appropriate because the output has no meaning to someone else. ANOVA tables should be nicely formatted in a proper table with the correct column names.

A results sentence needs at a minimum:

 * The main result
 * The name of the statistical test
 * The test statistic and degrees of freedom
     * Df can be written as a subscript to the test statistic (e.g. $t_{14}$) or reported as df = 14.
     * F statistic need the degrees of freedom for the within & among error. E.g. $F_{1,25}$
 * The P value
     * Really small or large P values can be summarised. E.g. P < 0.001
 
P values should never be reported on their own - they are also meaningless without the test statistic and context.

All the above information is given to you in `summary` or the ANOVA table. The degrees of freedom of a linear regression are found at the bottom of `summary` (`Residual standard error`).

```{r conclusion}
question("Based on the summary of the linear regression you just did, which is the most appropriate sentence reporting the results?",
         answer("There is an effect of shell width or colour on shell length (linear regression, $t_{197}$ = 16, P < 0.01)", correct = TRUE, message = random_praise()),
                  answer("There is an effect of shell width or colour on shell length (linear regression, $t$ = 16, P < 0.01)"),
         answer("There is an effect of shell width or colour on shell length ($t_{197}$ = 16, P < 0.01)"),
         answer("There is an effect of shell width or colour on shell length (P < 0.01)"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
         )
```


***

### Practice: Evaluating predator-prey interaction responses with additive models

Remember our predator-prey interaction response dataset has a **categorical predictor variable** for the foraging strategy of an individual: whether or not we used a lid with the jar. If we did a simple linear regression with one predictor variable, we would have ignored foraging strategy meaning we have **pooled** observations for foraging strategy.

By pooling foraging strategy, any variation in the data generated by foraging strategy is **unaccounted for**. Unaccounted variation may increase our chance of making a Type I or II error because foraging strategy **confounds** the effect of prey density. Phrased differently, the effect of prey density on numbers of prey captured is **masked** by the effect of foraging strategy. Thus, in this case, using a simple linear regression on a dataset with two or more predictor variables is not the best course of action for explaining as much variation in the data as we can.

If we want to see whether using a lid has an effect on our predator-prey interaction response, we need to include the variable in our linear model *in addition to our original predictor variable, prey density*.

To refresh your memory:  
H0: There is no difference in the number of prey captured with prey density between jars with and without a lid      
H1: There is a difference in the number of prey captured with prey density between jars with and without a lid

***

#### Characters or factors? Ordering categorical data

The jar without a lid treatment is coded as `no_lid` and the jar with a lid treatment is coded as `yes_lid`. We can also think of jar with a lid as our experimental control and the jar without a lid as our experimental treatment. 

As R shows you the difference between levels of a treatment and estimates these parameters based on **alphabetical ordering** of the levels, it may be helpful to think of the first level of a treatment and the first intercept and slope estimate as the control of your experiment. In other words, like a baseline you are comparing to since this is what R is doing when evaluating the one sample t test on parameter estimates.

At the moment, foraging strategy is classified as a **character** vector (see `str(class_data)`) and, our "control" group (`yes_lid`) is alphabetically after `no_lid`, so we have the reverse scenario. Under the hood, R will convert the `foraging_strategy` variable from a character vector to a **factor** when fitting the linear regression. Both characters and factors are ways R stores categorical data, the difference being that factors have a specific order.

We will not do anything about this for now. R will use `no_lid` as the baseline contrast for $\beta_0$ and $\beta_1$. We just need to keep the order in mind when parameterising.

If you needed to change the order of the sub-groups you will need to change the variable to a factor (`as.factor`) and define the order of levels using `levels = c("level1", "level2")`. If we did this, then the slope coefficient refers to `yes_lid` and the final estimated coefficient are exactly the same, just reversed.

***

Time to use what you've learnt about additive models on your dataset! There's no need to reload and prepare your data again - you've already done it so it should be in `R`'s memory.

> Answer the CA

***

## Part B: Interactive models

> In multiplicative models, both the slope and the intercept are estimated for each level of the factor

Thus, the slopes of the model represent a random variable so it's sometimes called an **random slopes model**.

We can write the full model $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$ in the partial equation form as:

$$ shell \space length = \beta_{0_{colour}} + \beta_{1_{colour}} shell \space width + \varepsilon$$
Now, both $\beta_0$ and $\beta_1$ vary according to colour - We expect two lines with *different* intercepts and *different* slopes.

```{r quiz3}
question_text(
  paste("How many partial regression lines are fitted to our crab dataset when we do a multiplicative model?"),
  answer("2", correct = TRUE, message = "Two lines. One for orange crabs and one for blue crabs. Each with different slopes and intercepts"),
  allow_retry = TRUE,
  incorrect = "Incorrect. Think about what the mathematical linear model is saying and try again"
)
```

***

### Hypotheses of a multiplicative model

Because both the slopes and intercepts vary, the way we write our hypothesis changes compared to the fixed model. Now we need to *describe the relationship between the two predictors on the outcome of the response variable*. In other words, the interaction between the predictors is the main relationship of interest.

```{r hypotheses, echo=FALSE}
quiz(caption = "Can you work out the hypotheses?",
     question("What is our null hypothesis?",
              answer("The effect of shell width on shell length is not dependent on the colour of the crab", correct = TRUE, message = random_praise()),
              answer("The effect of shell width on shell length is dependent on the colour of the crab"),
              answer("There is a relationship between shell length and shell width"),
              answer("There is no relationship between shell length and shell width"),
              incorrect = "Incorrect. Think about what the wording implies",
              allow_retry = TRUE,
              random_answer_order = TRUE
     ),
     question("What is our alternative hypothesis?",
              answer("The effect of shell width on shell length is not dependent on the colour of the crab"),
              answer("The effect of shell width on shell length is dependent on the colour of the crab", correct = TRUE, message = random_praise()),
              answer("There is relationship between shell length and shell width"),
              answer("There is no relationship between shell length and shell width"),
              incorrect = "Incorrect. Think about what the wording implies",
              allow_retry = TRUE,
              random_answer_order = TRUE
     )
)
```

Another way of phrasing the interaction is "there is an interaction between shell width and colour on shell length" or "there is no effect of shell width or colour or their interaction on the length of crab shells".  

Interactions imply that the effect of colour and shell width have **multiplicative** effects on shell length when considered together. This effect could be *antagonistic* or *synergistic.*

***

### Linear models with interactions in R

> The sturcture of the `lm()` function for an interactive model is the same as the additive model but now the relationship between the predictor variables is denoted by `*` to indicate the interaction. 

It's time to do the linear model.

*Change this additive linear model function into a multiplicative linear regression. Press run.*
```{r random, exercise = TRUE, exercise.lines = 5}
lm(CL ~ CW + sp, crabs)
```
```{r random-hint}
What's the difference in the R formula between an additive model and a multiplicative model?
```
```{r random-solution}
lm(CL ~ CW * sp, crabs)
```

As expected, we see that there are **four** coefficients. From left to right they are $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$ in the full model $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$.


```{r coef-quiz, echo=FALSE}
question("Which coefficient is present in a full model that is not present in an additive model?",
         answer("$\\beta_0$"),
         answer("$\\beta_1$"), 
         answer("$\\beta_2$"), 
         answer("$\\beta_3$", correct = TRUE, message = random_praise()),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

***

### Parameterising the model

The process of parameterising the model is *exactly* the same as we've done before but now we have *four* coefficients representing the slopes and intercepts for our two regression lines: $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$. So you should be expecting what's coming up. 

> Another way of denoting interactions in R is `:` so `X1:X2` means this is the interaction between `X1` and `X2`

```{r param-quiz, echo=FALSE}
crabs_int
question("What do the estimated coefficients for spO and CW:spO represent?",
         answer("The difference in the slope or intercept for orange crabs compared to blue crabs", correct = TRUE, message = random_praise()),
         answer("The difference in the slope or intercept for blue crabs compared to orange crabs", message = "Incorrect. Remember R is calculating blue crabs first before orange crabs because they are in alphabetical order"), 
         answer("The estimated slope or intercept for orange crabs"), 
         answer("The estimated slope or intercept for blue crabs"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

In the additive model we only needed to manually calculate the intercept by adding $\beta_0$ and $\beta_2$. Now we need to manually calculate the slope **and** intercept.  

Let's start with the intercept.

***

#### The intercept

Parameterising the intercept is exactly the same as the additive model - `(Intercept)` and `spO` mean the same thing as before - the estimated intercept for the baseline group ($\beta_0$) and the difference in the estimated intercept between blue and orange crabs ($\beta_2$). So we should be able to do this right away.

```{r quiz-slopes}
coef <- round(coef(crabs_int),2)
print(coef)
quiz(caption = "Use the coefficients above to answer the following questions:",
     question_text(
       paste("What is the intercept for blue crabs?"),
       answer(paste(coef[1]), correct = TRUE, message = "(Intercept) refers to the intercept for blue crabs. Notice that it is a different value to the additive model."),
       allow_retry = TRUE,
       incorrect = random_encouragement()
     ),
      question_text(
       paste("What is the intercept for orange crabs?"),
       answer(paste(coef[1] + coef[3]), correct = TRUE, message = "Notice that it is a different value to the additive model."),
       allow_retry = TRUE,
       incorrect = "Remember what we did for parameterising the additive model"
     )
)
```

***

#### The slope

Parameterising the slope is the same process as the intercept.

> The coefficient estimate `CW:spO` is the **difference in the value of the slope for orange crabs relative to the slope for blue crabs** `CW`. This is $\beta_3$.

Remember: blue is alphabetically before orange so R calculates the variances for blue crabs first, then orange crabs.  

We are given the slope for blue crabs already. We need to work out the slope for orange crabs by adding the coefficient estimates together.

What we need:  
Parametrised slope for orange crabs = Slope for blue crabs $\beta_1$ + difference in slope for orange crabs compared to blue crabs $\beta_3$  

Information provided from the `R` output:  
Slope for blue crabs = `r round(coef(crabs_int)[2],2)`  
Difference in slope for orange crabs compared to blue crabs = `r round(coef(crabs_int)[4],2)`

```{r orange-slope}
question_text(
  paste("Use the above information to calculate the slope for orange crabs"),
  answer(paste(round(coef(crabs_int)[2],2) + round(coef(crabs_int)[4],2)), correct = TRUE, message = random_praise()),
  allow_retry = TRUE,
  incorrect = "Keep trying - you have all the information already"
)
```

***

### The final parameterised models

Did you get the parameterised partial regression equations below?  

*blue crab shell length* = `r round(coef(crabs_int)[1],2)` + `r round(coef(crabs_int)[2],2)` *shell width*  
*orange crab shell length* = `r round(coef(crabs_int)[1] + coef(crabs_int)[3],2)` + `r round(coef(crabs_int)[2] + coef(crabs_int)[4],2)` *shell width*

The good news is that was as hard as it gets in this module.  

***

### Predicting new values

Now to use the linear regression to predict the length of crab shells. This is exactly the same process as we've done already and is straightforward once you have your fully parameterised equations.

```{r predict-random}
ccoef <- round(coef(crabs_int),2) 
ccoef_spO <- round(coef(crabs_int)[1] + coef(crabs_int)[3],2)
ccoef_CWspO <- round(coef(crabs_int)[2] + coef(crabs_int)[4],2)

blue <- sample(10:70,1)
ans_blue <- round(ccoef[1] + (ccoef[2] * blue), 1)

orange <- sample(10:70,1)
ans_orange <- round(ccoef_spO + (ccoef_CWspO * orange), 1)

###
size <- sample(10:70,1)
size_blue <- ccoef[1] + (ccoef[2] * size)
size_orange <- ccoef_spO + (ccoef_CWspO * size)
ans_size <- round(size_orange - size_blue, 1)
###
quiz(caption = "Test yourself using the equations above with coefficients to two decimal places",
      question_text(
        paste("What is the carapace length of a blue crab with a carapace width of ", blue, " mm? To 1 decimal place"),
        answer(paste(ans_blue), correct = TRUE, message = random_praise()),
        allow_retry = TRUE,
        incorrect = random_encouragement()
      ),
      question_text(
        paste("What is the carapace length of a orange crab with a carapace width of ", orange, " mm? To 1 decimal place"),
        answer(paste(ans_orange), correct = TRUE, message = random_praise()),
        allow_retry = TRUE,
        incorrect = random_encouragement()
      ),
      question_text(
        paste("How much longer is an orange crab than a blue crab with a shell width of ", size, " mm? To 1 decimal place"),
        answer(paste(ans_size), correct = TRUE, message = random_praise()),
        allow_retry = TRUE,
        incorrect = random_encouragement()
      )
)
```

***

### Evaluating hypotheses

*Complete the code to get the `summary()` of our multiplicative model*
```{r summary-random, exercise = TRUE, exercise.lines = 5}
lm(CL ~ CW * sp, crabs)
```

> H0: There is no effect of shell width or colour or their interaction on shell length  
> H1: There is an effect of shell width or colour or their interaction on shell length

Like with the additive model, the hypothesis tests on the coefficient estimates for the slope are the important ones:

* `CW` - This tests whether the slope of blue crabs is different to 0
    * This gives an indication of whether shell width can predict shell length
* `CW:spO` - This tests whether the slope for orange crabs is different to blue crabs
    * This tests whether there is an interaction between colour and shell width
    * If this is non-significant, then a multiplicative model is overly complex to describe our biological pattern (because it uses too many parameters) and perhaps an additive model is a better descriptor. Aiming to minimise the number of parameters used in a linear model is the statistical concept of **model parsimony**. 


```{r random-test}
quiz(caption = "Answer the following questions:",
  question_radio(
    "Based on the test on the slope coefficient above, would you accept or reject the null hypothesis?",
    answer("Accept"),
    answer("Reject", correct = TRUE, message = "The P value is less than 0.05"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_checkbox(
    "What does the multiplicative linear regression tell you about the relationship between our variables? Select all that apply",
    answer("There is no interaction between shell width or colour on shell length"),
    answer("There is an interaction between shell width or colour on shell length", correct = TRUE),
    answer("The relationship between shell length and shell width is different between blue and orange crabs", correct = TRUE),
    answer("The effect of shell width on shell length is dependent on the colour of the crab", correct = TRUE),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

***

### The multiple regression ANOVA table

We can look at the ANOVA table for an additive or interactive linear model. If it's an interactive model the amount of variation in shell length attributed to the interaction between colour and shell width will also be displayed.

Here is the ANOVA table for the interactive linear model (the more complex one):

```{r anova-table-random}
fixp <- function(x, dig=3){
  x <- as.data.frame(x)
  
  if(substr(names(x)[ncol(x)],1,2) != "Pr")
    warning("The name of the last column didn't start with Pr. This may indicate that p-values weren't in the last row, and thus, that this function is inappropriate.")
  x[,ncol(x)] <- round(x[,ncol(x)], dig)
  for(i in 1:nrow(x)){
    x[i,ncol(x)] <- ifelse(x[i,ncol(x)] == 0,
                           paste0("< 0.", paste0(rep(0,dig-1), collapse=""), "1"),
                           x[i,ncol(x)])
  }
  x
}
knitr::kable(fixp(anova(crabs_int)), digits = 3)
```

This is what the above ANOVA table is showing:  

|  Source of variation  | SS | df | MS | F | P |
|:---------------------:|:--:|:--:|:--:|:-:|---|
| Factor X1 | SSR of X1 | number of levels of X1 - 1 | |  |   |
| Factor X2 | SSR of X2 | number of levels of X2 - 1 ||   |   |
| Factor X1 x X2 | SSR of X1 & X2 | df of X1 x df of X2 ||   |   |
| Within error | SSE | levels of X1 x levels of X2 X (number of observations - 1) | |   |   |
|  Total error   | SSY | (levels of X1 x levels of X2 X number of observations) - 1 |    |   |   |

> Variance of predictors (SS) is partitioned out from total SS in the order it is entered in to R

We can also use the F test on the interaction in the ANOVA table to test the null hypothesis that the effect of shell width on shell length is not dependent of the colour of the crab.

```{r quiz4}
question_radio(
  "Based on the F test on the interaction in the above ANOVA table, would you accept or reject the null hypothesis?",
  answer("Reject", correct = TRUE, message = "The P value is less than 0.05. There is an interaction between colour and shell width"),
  answer("Accept"),
  allow_retry = TRUE,
  incorrect = random_encouragement()
)
```


***

### Practice: Evaluating predator-prey interaction responses with multiplicative models

We'll rephrase our hypotheses slightly to make the effect of the interaction explicit:   

H0: The relationship between prey captured and prey density is the same between jar type  
H1: The relationship between prey captured and prey density is different between jar type

Time to use what you've learnt about interactive linear models on your dataset!

We should point out that the Type II model technically does not **deal with multiple variables** so using a multiplicative models are instead a statistical representation of the data, rather than a theoretic model.

> Answer the CA. Save and submit your answers. That is the end of the CA

***

### End of Part A

That's multiple regression. It's very similar to simple regression but the additional predictor variable makes the interpretation of R output a little more complex. But completely do-able! That's about as challenging as this module will go! Well done!

> Congratulations for making it here! Pat yourself on the back.

***

## Part C: Visualising data

Visualising data is very important for understanding our data and for communicating our data to others. For example, in a written report.

`plot` is the general plot function. Boxplots (`boxplot`) and bar plots (`barplot`) have their own plotting commands. You can add error bars using `arrows`.

### Theory

> The general formula to plot a scatter graph is `plot(response ~ predictor, data)`

If our data is categorised (e.g. `sp` in `crabs` has `B` or `O`), then we need to plot our points for each group separately. We can do this by calling a plot with no points using `plot(response ~ predictor, data, type = "n")` where `"n"` tells `R` not to plot anything. Then we add points manually with `points(response ~ predictor, data[data$group == "subsetA",])` for each sub-category.

We need to subset our data (like we did in previous pracs) so that `R` only plots the sub-categories of data.

*Using the crabs dataset, modify the code below to plot crab shell length (response) against crab shell width (predictor) for __only blue crabs__.*
```{r graph, exercise=TRUE, exercise.lines = 5}
plot(response ~ predictor, data, type = "n")
points(response ~ predictor, data[data$group == "subsetA",])
```
```{r graph-hint-1}
Have you replaced all the terms with the corresponding one from the crabs dataset?
```
```{r graph-hint-2}
Are you cases correct?
```
```{r graph-hint-3}
You should have two lines of code.
```
```{r graph-solution}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
```

If you are correct, your graph should match the one below:
```{r crabsol}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
```

*Now add the orange crabs to plot all the data - You have to start the code from the beginning.*
```{r orangec, exercise=TRUE, exercise.lines = 5}
plot(response ~ predictor, data, type = "n")
points(response ~ predictor, data[data$group == "subsetA",])
```
```{r orangec-hint-1}
Have you replaced all the terms with the corresponding one from the crabs dataset? You can copy your code from before so you don't have to type it out again
```
```{r orangec-hint-2}
Are you cases correct?
```
```{r orangec-hint-3}
You should have three lines of code
```
```{r orangec-solution}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
points(CL ~ CW, crabs[crabs$sp == "O",])
```

Uh oh! We cannot distinguish between the colours of crabs! Time to learn more `R` to add more features to the graph.

***

#### Colours

Colour is important when presenting data. Are the colours meaningful? Are they necessary? Can they be clearly distinguished? Are they appropriate for screens, for printing, or accessible for colour-blind people?

> Red-Green colour blindness is the most common form of colour blindness. A simple guideline is to avoid using red and green together where ever possible. Blue and orange are better constrasting colours (that's why they are common colour schemes for movie posters).

Colour in `R` is defined by `col`. So in a graph if I wanted to change the colour of the points from black (default) to red then I can either call `col = "red"` or `col = 2` as an argument within the `points()` function, because red is the second colour in the default R colour palette (black is 1). There are lots of colours to choose from (Google it for a full list). R also accepts hexidecimal RGB colour codes for custom colours (e.g. black is #000000).

*Change the colour of the points to their relevant colour (blue or orange)*
```{r col, exercise=TRUE, exercise.lines = 5, exercise.eval = TRUE}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
points(CL ~ CW, crabs[crabs$sp == "O",])
```
```{r col-hint-1}
col stands for colour and tells R what colour to pick. colours are in lowercase
```
```{r col-hint-2}
Have you remembered to use quotation marks? You can also use numbers in the default palette.
```
```{r col-solution}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",], col = "blue")
points(CL ~ CW, crabs[crabs$sp == "O",], col = "orange")
```

***

#### Shapes

Shapes are important because it makes the points stand out. It can also be used to distinguish between groups of data on the same graph. Sometimes it's necessary to change colour *and* shape to make it easier to distinguish between groups - redundancy is acceptable and encouraged in graphical design.

The shape of the points are coded `pch = <number>` as an argument within `plot()` or `points()`. There are lots of options designated a number from 1 to 25. (Google "R pch" for the full list). 1 is the default open circle. A filled circle is 16.

*Change the shape of the points from open to filled circles.*
```{r pch, exercise=TRUE, exercise.lines = 5, exercise.eval = TRUE}
plot(CL ~ CW, crabs, pch = 1)
```
```{r pch-hint-1}
pch tells R what shape to pick. It accepts a number
```
```{r pch-hint-2}
What is the number for filled circles?
```
```{r pch-solution}
plot(CL ~ CW, crabs, pch = 16)
```

You can combine all the code above to change the colour, shape and axes labels.

This graph is fine but if we were to put this in a professional scientific paper or report there are a few missing elements and we may want to customise the aesthetics for a prettier graph.

***

#### Axes labels

The `R` code to change the labels on a graph are `xlab` and `ylab` for the x axis and y axis, respectively. These are called within the `plot()` function like:
```
plot(response ~ predictor, data, xlab = "label", ylab = "label")
```

*Change the x axis label of our `crabs` plot to "Width" instead.*
```{r axes, exercise=TRUE, exercise.lines = 5, exercise.eval = TRUE}
plot(CL ~ CW, crabs)
```
```{r axes-hint-1}
The code to define the x axis label is xlab
```
```{r axes-hint-2}
Have you  remembered to use quotation marks?
```
```{r axes-solution}
plot(CL ~ CW, crabs, xlab = "Width")
```

In practice, we want our figures to be informative and be understandable independently of the main text. This means having informative labels and including units.

*There are __two mistakes__ in the following code to change the axis labels to "Width" or "Length" AND include units (mm) in both the x and y axis. Fix it and run the code*
```{r units, exercise=TRUE, exercise.lines = 5}
plot(CL ~ CW, crabs, ylab = "Width (mm)", ylab = "Length (cm)")
```
```{r units-hint-1}
The code to define the x axis label is xlab
```
```{r units-hint-2}
Have you remembered to use quotation marks?
```
```{r units-solution}
plot(CL ~ CW, crabs, xlab = "Width (mm)", ylab = "Length (mm)")
```

***

#### Regression lines

Once we've done the hard work to do a linear regression, it's nice to add it to our graph so we can see how it fits to the data. A linear regression is always a linear line (straight line) so we can use the code to plot a straight line.

The formula to plot a straight line is `abline(intercept, slope)` because it plots a line from a to b. The intercept is the first value, the slope is the second value. You need to plot the data first before adding additional lines.

Here are the coefficients for a simple linear model for the crabs showing the intercept and slope respectively:
```{r coef-reminder}
round(crabs_lm$coefficients, 1)
```

*Complete the `abline()` formula to plot our regression line then press run.*  
```{r abline, exercise=TRUE, exercise.lines = 5}
plot(CL ~ CW, crabs)
abline()
```
```{r abline-hint}
The slope and the intercept of our model was calculated by lm(CL ~ CW, crabs)
```
```{r abline-solution}
plot(CL ~ CW, crabs)
abline(-0.662, 0.9)
```

The same code is used to plot either regression line for a multiplicative regression - you just need to use the slope and intercept values you want!

You can also change the colour, type and width of the line as arguments within `abline()`.

* `lwd = <number>` is line width. The default is 1. Values greater than 1 are a thicker line.
* `lty = <number>` is line type. The default solid line is 1. Different types of dashed or dotted lines are numbers 2 to 6.

***

#### A final graphing test

Let's integrate everything we've leant today and plot the graph of our interactive linear model with the correct regression lines.

```{r final-graph, fig.cap= "A complete graph for a scientific report"}
plot(CL ~ CW, crabs, type = "n", xlab = "Width (mm)", ylab = "Length (mm)")
points(CL ~ CW, crabs[crabs$sp == "B",], col = "blue", pch = 16)
points(CL ~ CW, crabs[crabs$sp == "O",], col = "orange", pch = 16)
abline(-0.364, 0.876, col = "blue")
abline(0.077, 0.894, col = "orange")
```

Here are the coefficients for the interactive linear model:  
```{r int-coef}
round(crabs_int$coefficients, 1) 
```

*Change the basic crabs graph to match the graph above - this graph is suitable for a scientific report*
```{r final-graph-test, exercise=TRUE, exercise.lines = 6, exercise.eval = TRUE}
plot(CL ~ CW, crabs)
```
```{r final-graph-test-hint-1}
You need to change the axes labels
```
```{r final-graph-test-hint-2}
You need to plot the points for different coloured crabs separately on a blank graph
```
```{r final-graph-test-hint-3}
You need to change the shape type
```
```{r final-graph-test-hint-4}
You need to change the point colours
```
```{r final-graph-test-hint-5}
You need to add the regression lines separately, including the aesthetics
```

#### Other variables in `plot`

**Borders**   
A border around the plot is plotted by default. This is dictated by the default `bty = "o"`. You can remove the border entirely with `bty = "n"` and you can plot only the bottom and right border (axes) with `bty = "l"` - that's a lowercase L, not a number 1. Borders also apply to legends.

**Legends**  
Figure legends can be added using `legend`. Check out the help file for details because you need to set:

 * Where to put the legend
 * The text of the legend
 * The colours
 * The lines or points used
 
**Title**  
Figure titles can be set using `main = "<title>"`. There are other ways of captioning figures too, like subtitles.

**Lines**  
`type` tells R what kind of plot you want. R plots points (`type = "p"`) by default but can also plot lines (`type = "l"`), [lowercase L]. You can plot a combination with `type = "b`.

`lines` is the equivalent to `points` for plotting individual lines.

***

#### Captions

The last thing every scientific graph needs is a caption. The caption needs to describe the graph and explain every aspect of the graph to the reader *independently* of the main text.

* What is the overall relationship?
* What is on the x axis? What is on the y axis?
* What is the sample size?
* What do the colours, lines or points represent?

Often a single sentence saying "Figure 1. A graph of the response against the predictor" is not enough information in a professional report.

***

### Practice: Graphing results with a caption

Use your new found knowledge of making graphs in R to make a plot of our results from our predator-prey interaction response including appropriate axes labels with units. The aesthetics are up to you. Write a caption as well.

You can show it to your demonstrator for feedback.

***


##

That's the end of Part C. Take a break. Stand up. Shake your limbs. Breathe.

```{r break-quiz}
question(
  "Who lives in a pineapple under the sea?",
  answer("Haven't you asked this already?", correct = TRUE),
  answer("Patrick Star"),
  answer("Squidward Tentacles"),
  answer("Sandy Cheeks"),
  random_answer_order = TRUE
)
```

> There are no CA questions for this section

***

## End

That's the end of the prac. We've covered a lot of practical skills that you can use in your final report, in the future and in your other modules. Just as you should build upon what you've learnt in previous modules in this one.

Here's a summary of what we've done:

 * Run a linear regression in R with biological data - simple regression, additive regression & interactive regression
 * Parameterise a linear regression from R output 
 * Use linear models to predict new values
 * Interpret linear regression output to test hypotheses
 * Make a graph in R with appropriate labels
 * Write a figure caption
 
Over the course of the last practical and this one, we have done a crash course in the Scientific Method from formulating a hypothesis from a biological system (infection or predator/prey response) to evaluating hypotheses and every thing in between.
