---
title: "Statistical modelling Part 2"
output: 
    learnr::tutorial:
      theme: readable
      progressive: true
      allow_skip: true
runtime: shiny_prerendered
subtitle: "2022"
description: "2022 BYU22S01 Practical"
---


```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE, comment = NA)
crabs <- MASS::crabs
crabs_lm <- lm(CL ~ CW, crabs)
crabs_add <- lm(CL ~ CW + sp, crabs)
crabs_int <- lm(CL ~ CW * sp, crabs)
palette(c("#0072B2", "#D55E00"))
tutorial_options(exercise.completion = FALSE)
test_data <- data.frame(concentration = sample(size = 10, 1:20))
```

## How to use this worksheet

This worksheet is an interactive document run through RStudio. It is made using the R package `learnr`, which you installed before the prac. The worksheet has interactive code chunks that you can use to enter `R` code and quizzes to test yourself - which makes this more fun than a regular word document or PDF and lets you practice R coding.

You can interact with this worksheet through the Tutorial tab in RStudio. This window can be resized and expanded. However, I recommend clicking the pop out button (in between the home icon and the stop icon) to open the tutorial in a separate window. 

***

**Code chunks**  
Code chunks are independent of the main RStudio Environment. Anything you type here will not be saved to memory, but you won't need to. Try it:  
*Here's a simple exercise with an empty code chunk provided for entering the answer. Click __Run Code__ to run the code. The __Hint__ button will tell you a hint. If you are really stuck, the last hint may be the solution.*

Write the R code required to divide 10 by 5 (the answer should be 2):

```{r test, exercise=TRUE, exercise.lines = 5}

```

```{r test-hint}
You need a / somewhere. You do not need =
```

```{r test-solution}
10/5
```

You can use RStudio and R scripts at the same time for trying things out or taking notes. 

***

**Quizzes**  
**Quizzes here are not part of the CA.** The questions are meant to check you understand the material and your answers are not visible to anyone else. The CA is accessed separately.

*Here's a quiz. Try it out to see how it works. Click __submit__ and it will tell you what you got correct. What happens when you put an incorrect answer in?*
```{r test-quiz}
quiz(caption = "Answer the following questions:",
question(
  "What colour is a grey squirrel?",
  answer("Red"),
  answer("Brown"),
  answer("Grey", correct = TRUE),
  answer("Black"),
  allow_retry = TRUE,
  random_answer_order = TRUE
),
question_text(
  paste("What is the answer to the above coding exercise?"),
  answer("2", correct = TRUE),
  allow_retry = TRUE
),
question_checkbox(
  "Which of these are fish? Select all that apply.",
  answer("Seadragon", correct = TRUE),
  answer("Starfish"),
  answer("Jellyfish"),
  answer("Cuttlefish"),
  answer("Alligator gar", correct = TRUE),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
)
)
```

***

Well done!  
Your answers are saved if you open the tutorial again. You can reset your answers by clicking **Start Over** at the end of the menu. You can run these tutorials any time you like if you want to practice again. 

Although you can come back to this tutorial at any time, I recommend taking your own notes for your reference. There are also discussion questions in bold that you should think about or write down an answer.  

Click **Next Topic** to progress to the practical instructions.  
You can navigate between topics using the buttons or the menu.  

> You must complete all the exercises to move ahead.  
> Do not skip exercises - This is the "practice" of the practical

***

## Practical outline

Welcome to Part 2! We will continue to look at statistical modelling using the data we collected in the previous practical. 

In this practical we aim to take a practical and hands-on approach to learning statistical modelling by using R and a real dataset you collected. We focus on the practicalities of *applying* statistics (implementation & interpretation) - not on the theory or mathematics (which was covered earlier in semester).

**Learning objectives:**

 * Know how to parametrise a linear regression in R with biological data
 * Know how to use linear models to predict new values
 * Know how to interpret linear regression output to test hypotheses
 * Know what to include in a results paragraph
 * Know how to make a graph in R with appropriate labels and caption

> We have covered all of these concepts in the lectures or previous practicals. This prac will reinforce and revise these concepts and allow you to practise them under supervision from the demonstrators.

There are two goals here:

1. A theory section where we will recap the lecture content and go over the practicalities of linear regression in R step by step using an example dataset. There are code chunks and revision questions. You can retry questions you get wrong - learning is about the process not the marks!
2. A practice section where you will apply your new skills to the data you collected last practical. This is required to answer the CA. You will need access to the class dataset from the previous practical so save it somewhere you know.  

> Don't forget to answer the CA questions in a separate link. There are prompts that will tell you when to answer the questions. **I do not recommend skipping the theory before attempting the CA - the theory section will go through all the steps you need for analysing your data**.

The CA aims to assess your understanding of the concepts in this practical and your ability to apply the concepts in practice to new scenarios. The demonstrators aren't allowed to help you answer the CA directly but they can help you understand the theory parts so that you can answer the CA. 

***

## How to problem solve
### Some general advice for programming and learning

We don't expect you to be expert programmers but we *do* expect you to be able and willing to figure things out yourself (problem solving).

In programming, it's *really easy* to make mistakes that breaks code. That is no means a reflection on you or your ability to code/learn.  

If your code is not working take a moment to *breathe*. Then check for common, minor errors such as:

 * Spelling mistakes
 * Wrong dataset name
 * Wrong variable (column) name
 * Missing or wrong quotation mark
 * Missing bracket
 * Inconsistent cases (e.g. Uppercase)
 * Missed a step
 * Invalid syntax (e.g. spaces)
 * Duplicates of the same function with multiple errors - keep your scripts tidy!

These reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.
 
 > You can **and should** easily fix the above mistakes yourself!

The point of learning to code is to not treat statistical software like a black box where you cannot see inside and you don't know where the numbers came from.

**Do not take the code here for granted. One does not simply copy code from somewhere and blindly expect it to work out of the box.**
 
Programming is like learning the grammar and syntax of another language and writing a script is like writing a recipe for a computer. Each line of code has a purpose and meaning. Computers only do what they are told, the rest is up to you.

*** 

### Talk to a rubber duck

You need a clear idea of what you want to achieve in programming. Articulate in words to yourself what you aim to do, what you expect to see if it worked, what you did leading up to this point and what you think is happening to cause the error. This is called the rubber duck method - like you are explaining your problems to a rubber duck.

If there's an error message, read it. Error messages are the computer telling you what's wrong. Try googling the entire error message to see what other people have said. The entire error message is necessary, not just the last bit. 

> There's no shame in googling everying - that's what we do too and part of learning to work independently

Take the time to *synthesise* and *integrate* information so that you can apply it to *new scenarios* - that is the learning objective of your entire university degree. And what we will be doing here.

Properly read and digest the material you are given because almost all your assessments will test your reading comprehension and attention to detail. And there's nothing in this practical that wasn't covered earlier in semester and in the lectures.

Don't skim read. Take notes. Follow the written instructions.

 > Learning is about the process and about your growth, not the end result or your grade

One of the hard things about troubleshooting someone else's programming is that it's not clear what the problem is. When describing your problem, be comprehensive & give context. Show the entire error message, not just parts of it. Trace your steps leading up to it. 

The more information you provide, the more information and context people have to help you. That's why we might ask you to provide more information or ask further questions to troubleshoot. The first things we would check are the steps above (minor human errors), so if you've done those, then say what you've already checked and what causes you've ruled out. That gives us a starting point to assist you further.

Programming is **trial and error**. You shouldn't expect to get it on the first try. And that's OK - it's part of the process.

***

###

 > Take your time, work through the material in order and practice the basics before you progress to harder stuff. Refer to your lecture notes or previous practicals and integrate the information.   
 
 > Unless you are 100% confident with data handling and doing multiple regressions in R, do not skip ahead to answer your assessment on the fly.
 
 > Everything we do here is building on and applying what we've previously covered. There's nothing new here.
 
 
Let's try a problem solving exercise:  
*The following code should calculate the median of a column called `concentration` in a dataset called `test_data` but it doesn't work.*  
`median(concentration$test_data)`
```{r reading}
question(
  "Why doesn't the above code work?",
  answer("Spelling mistake"),
  answer("Wrong function structure", correct = TRUE, message = random_praise()),
  answer("Wrong function to calculate medians"),
  answer("Missing bracket"),
  answer("Missing quotation marks"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
)
```

*Enter in the correct code to calculate the median of `concentration`. `test_data` is already stored as an R object. Press Run Code.*
```{r reading-exercise, exercise = TRUE}

```

```{r reading-ans}
ans_conc <- median(test_data$concentration)
question_text("What is the median of concentration?",
  answer(paste(ans_conc), correct = TRUE, message = random_praise()),
  allow_retry = TRUE,
  incorrect = random_encouragement()
)
```

***

## Functional responses recap

Remember, in the previous practical we asked the question "How does prey density and foraging strategy affect the number of prey captured in a predator-prey scenario?". 

We have three variables of interest in our experimental design:

 * `prey_captured`: The response variable, continuous numeric variable
 * `prey_density`: The first predictor variable, continuous numeric variable
 * `foraging_strategy`: The second predictor variable, categorical variable with two sub-groups: `no_lid` and `yes_lid` for whether the jar had a lid or not.

We derived a mathematical expression of a predator-prey interaction (Type II model) called a functional response by turning our assumptions about a predator prey interaction into mathematical equations. You can refresh your memory from `vignette("functional_responses")`.

```{r recap, echo = FALSE}
quiz(caption = "In case you forgot",
     question("What is does $H_a$ represent?",
              answer("Prey density"),
              answer("Prey eaten", correct = TRUE),
              answer("Number of predators"),
              answer("Duration of the predator-prey interaction"),
              allow_retry = TRUE),
     question("What is does $H$ represent?",
              answer("Prey density", correct = TRUE),
              answer("Prey eaten"),
              answer("Number of predators"),
              answer("Duration of the predator-prey interaction"),
              allow_retry = TRUE),
     question("What is does $T_h$ represent?",
              answer("Prey density"),
              answer("Prey eaten"),
              answer("Number of predators"),
              answer("Time spent to capture one prey", correct = TRUE),
              allow_retry = TRUE),
          question("What is does $T_{total}$ represent?",
              answer("Prey density"),
              answer("Time spent to handle one prey"),
              answer("Time spent between each prey item"),
              answer("Total time spent foraging", correct = TRUE),
              allow_retry = TRUE))
```

The Type II model looks like this:

```{r funct_resp, echo=FALSE, fig.cap= "Type II functional response of an predator-prey response"}
prey <- seq(0, 60, 5)
type2 <- (prey*0.7)/(1+(prey*0.7*0.15))
plot(type2~ prey,
     type = 'l',
     lwd = 4,
     xlab = "Prey density", ylab = "Number of prey captured",
     xaxt='n', yaxt='n')
```

The equation of the model is:

$$H_a=\ \frac{a\times H\times T_{total}}{1+a\times H\times T_h}$$

The model is not linear.

```{r linear, echo=FALSE}
question("Why is the Type II model not linear?",
         answer("It has parameters that are divided by another parameter", correct = TRUE, message = random_praise()),
         answer("It has parameters that aren't exponents"),
         answer("It does not have a slope parameter"),
         answer("It does not have an intercept parameter"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

The aim of the practical was to **find the unknown values of search rate, $a$ and handling time, $T_h$** and we hypothesised that these values will change depending on the type of jar used. 

 > The process of estimating the value of unknown parameters in a statistical model is called **parameterising**.

Non-linear methods of parametrising these equations are beyond the scope of this module. But we can use algebra to make this equation linear and therefore we can use linear regression to get an estimate for the unknown parameters (see `vignette("TypeII_models")` to refresh your memory).

We then get a model:

$$\frac{1}{H_a}=\ \frac{1}{a}\times\frac{1}{H\times T_{total}}+\frac{T_h}{T_{total}}$$
This model has the form $Y = \beta_0 + \beta_1 X$ of a general linear model. 

```{r slope}
quiz(caption = "Answer the following questions:",
  question(
  "What does $\\beta_1$ represent?",
  answer("response variable"),
  answer("slope", correct = TRUE, message = random_praise()),
  answer("predictor variable"),
  answer("intercept"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
),
  question(
  "What does $\\beta_0$ represent?",
  answer("response variable"),
  answer("intercept", correct = TRUE, message = random_praise()),
  answer("predictor variable"),
  answer("slope"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  incorrect = random_encouragement()
))
```

The graph looks like this:
```{r tyepe2line, echo = FALSE, fig.cap= "Linearised type II functional response of an predator-prey response"}
prey <- seq(0, 0.25, 0.01)
type2 <- seq(0, 1, length.out = length(prey))
plot(type2 ~ prey , type = 'l', lwd = 4, ylab = "1/Ha, Prey captured", xlab = "1/H Total time, Prey density",
     xaxt='n', yaxt='n')
```

Now we can fit a linear model and find the values of search rate and handling time from the slope and intercept of the linear regression.

$$ \beta_1 = \frac{1}{a}$$
and
$$ \beta_0 = \frac{T_h}{T_{total}}$$

***

### Which linear model?

In the lectures, we saw that the structure of the appropriate linear model to fit depends on the characteristics of the data, the experimental design and the hypothesis. We looked at cases with one predictor variable (simple regression) and with two predictor variables (multiple regression). There are also two types of multiple regression: additive and interactive/multiplicative.

```{r quiz1}
quiz(
  question("What is the difference between a simple linear regression and a multiple linear regression?",
           answer("There is no difference"),
           answer("Simple linear regressions have two predictor variables, multiple linear regressions have one"),
           answer("Simple linear regressions have one predictor variable, multiple linear regressions have two or more", correct = TRUE, message = random_praise()),
           answer("Simple linear regressions have two response variables, multiple linear regressions have one"),
           answer("Simple linear regressions have one response variable, multiple linear regressions have two or more"),
           allow_retry = TRUE,
           random_answer_order = TRUE,
           incorrect = random_encouragement()
  ),
  question("What is the difference between an additive regression and a multipcative regression?",
           answer("There is no difference"),
           answer("Additive linear regressions have one predictor variables, multiplicative regressions have two"),
           answer("Additive linear regressions describe an independent effect of the predictors, multiplicative regression describe a dependency of the predictors on each other", correct = TRUE, message = random_praise()),
           answer("Additive linear regressions describe a dependency of the predictors on each other, multiplicative regression describe an independent effect of the predictors"),
           allow_retry = TRUE,
           random_answer_order = TRUE,
           incorrect = random_encouragement()
  ),
  question("Which type of regression so you think we can fit to this data?",
           answer("Any type of regression will do"),
           answer("Either type of multiple regression", correct = TRUE, message = "Technically, we could fit any of these to the data, but the interpretation and appropriateness varies"),
           answer("Additive multiple regression"),
           answer("Interactive multiple regression"),
           allow_retry = TRUE,
           random_answer_order = TRUE,
           incorrect = random_encouragement()
  )
)
```

If we did a simple linear regression with one predictor variable, we would have ignored foraging strategy meaning we have **pooled** observations for foraging strategy.

By pooling foraging strategy, any variation in the data generated by foraging strategy is **unaccounted for**. Unaccounted variation may increase our chance of making a Type I or II error because foraging strategy **confounds** the effect of prey density. Phrased differently, the effect of prey density on numbers of prey captured is **masked** by the effect of foraging strategy. Thus, in this case, using a simple linear regression on a dataset with two or more predictor variables is not the best course of action for explaining as much variation in the data as we can.

If we want to see whether using a lid has an effect on our predator-prey interaction response, we need to include the variable in our linear model *as well as our original predictor variable, prey density*.

***

#### Additive or interactive?

 > The wording of the hypothesis will dictate the appropriate linear model and our expectations about the graph.

Recall, we expect an initial *positive* relationship between prey density and the number of prey captured - the number of prey captured will increase with prey density until there are too many prey for a predator to handle and the number of prey captured will plateau. We can see this relationship in the previous graphs. So this needs to be in our hypothesis but we need to decide the nature of the relationship between the predictors.

```{r interac}
quiz(
  question_radio(
    "Following the theory underlying the Type II functional response, do you think the effect of prey density and foraging strategy on the number of prey captured are dependent on each other?",
    answer("Yes, they are dependent on each other"),
    answer("No, foraging strategy does not influence prey density", correct = TRUE, message = random_praise()),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_radio(
    "Thus do you think the above answer represents an additive or interactive linear model?",
    answer("Additive model", correct = TRUE, message = random_praise()),
    answer("Interactive model"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

We could fit an interactive model to our data if we were more interested in finding a mathematical description of our data that **explains the most variation in prey captured using the simplest model possible** (model parsimony). So using a multiplicative models are instead a statistical representation of the data, rather than a theoretic model. 

Both are types of statistical models and valid in their own right but in this case an interaction doesn't quite match the goals of constructing a theoretic model of predator-prey interactions from fundamental observations.

***

### Hypotheses

Foraging strategy determines the amount of prey captured but does not affect the number of prey present. And prey density determines the number of prey captured but does not influence the foraging strategy. This is based on our assumptions about the predator-prey interaction - the predator does not change foraging behaviour.

If the predator changed their foraging behaviour then there would be an interaction between the two predictor variables. This could be realistic. For example, animal predators might use different foraging strategies under different prey densities or they may switch to another strategy when prey density reaches a certain threshold. In fact, this is a Type III model, a sigmoidal function.

An appropriate set of hypotheses are:  

H0: The inverse of the number of prey captured increases with the inverse of prey density but does not differ between foraging strategies  
H1: The inverse of the number of prey captured increases with the inverse of prey density and is higher with the more efficient foraging strategies but the rate of prey captured does not vary

 > Think carefully how these sentences are worded and what you would expect to see in a graph.

***

## Additive linear models

As we saw in the lectures, the theory behind all linear regressions is the same no matter how many predictor variables or interactions you have.  

 > Variance in $Y$ is partitioned in the order it is presented and in alphabetical order (unless otherwise asked to) in R 
 
To practice fitting linear regression in `R`, we will use the dataset `crabs` - see `help(crabs)` for more information. You can also check the data using `str(crabs)`. This dataset is provided in `R` in the package `MASS`. I have already loaded the data for you within the tutorial so you don't need to do it but you will need to load it via `library(MASS)` if you want to try code in a script. What happens in the interactive tutorial is independent of R's Environment.

There are three variables we will look at (carapace is the zoological term for shell):

 * `CL`: Carapace length (mm) - our response variable
 * `CW`: Carapace width (mm) - our predictor variable
 * `sp`: Colour morph (`B` or `O` for blue or orange crabs)

The code we use here can be applied to any dataset. You can try it with any biological system of your choice in your own time. For example, `airquality` has environmental data or the Penguins [dataset](https://towardsdatascience.com/penguins-dataset-overview-iris-alternative-9453bb8c8d95) about penguins can be downloaded as a package.
 
We can ask the question "Does the relationship between shell length and width differ between colour morphs?". We can also phrase this as "Does the relationship between shell length and width depend on the colour of the crab?". Compared to a simple linear regression with one predictor variable (e.g. `CW`), this is a more complex question needing a more complex experimental design (two predictor variables) and thus a more complex statistical analysis.

> Multiple regression allows us to ask "Does including information about our predictor variables improve our ability to detect/understand trends in our response variable?"

"Bigger" crabs are expected to be larger in width and length so we expect a positive relationship between these variables. Whether this relationship is *also* dependent on the colour of the crab is what we can find out! We don't expect colour morphs to influence carapace width, i.e. crabs don't change colour as they get bigger, so an additive model fits with our understanding of the biological system.

You'll notice that the structure of this data is intentionally the same as our functional response experiment. For the CA you will need to apply what you've learnt here to your own data.

It always helps to see a graph of the data:
```{r crab-graph, fig.cap= "Do you think the relationship between shell length and width differs with the colour of the crab?"}
plot(CL ~ CW, crabs, pch = 20, col = sp, bty = "l")
legend("bottomright", bty = "n", c("Blue", "Orange"), pch = c(20, 20), col = c(1,2))
```

***

### Mathematical expression

Mathematically, an additive model is:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon$$

Where $X_1$ is our first predictor variable and $X_2$ is our second predictor variable. 

$\beta$ are the regression coefficients (also called terms) describing the variation in $Y$ attributed to each coefficient. The additive model has three terms that are unknown to us: $\beta_0$, $\beta_1$ and $\beta_2$. R's job is to find the values of these coefficients from empirical data.

$\varepsilon$ is the random error (residual error) not accounted for by the model - it's always part of the equation but it's often ignored, much like $c$ in integration.

 > In additive models, the effect of one predictor on the response variable is **additive** or **independent** of the other.

There is **no interaction** between the predictor variables colour or shell width (i.e. $\beta_3 = 0$). Thus the two fitted lines have the same slope ($\beta_1$) - hence they are sometimes called *fixed slopes* models. The model is described as a "**reduced**" model because it does not contains all possible terms - all predictor variables and all possible combinations of their interactions. 

 > Interactive models are considered "**full**" models if they contain all interactions in a fully crossed design.

To estimate the $\beta$ coefficients, the ordinary least squares regression splits the dataset into its predictor variables and fit a model to each component - this is called **partial regression**. The goal is to assign as much variation in the data as possible to each predictor variable. This process requires a baseline variable that sets the contrast for assigning variation (i.e. what $\beta_0$ represents).

**Dummy variables** are used with a **categorical predictor** to set the baseline for the ordinary least squares regression. Since Blue is alphabetically before Orange, Blue is the baseline contrast used in the partial regression process and assigned the dummy variable 0 in `crabs`. 

Thus for a blue crab, the dummy variable is 0:  
$$CL = \beta_0 + \beta_1 CW + \beta_2 sp \times 0 + \varepsilon$$ 
simplifies to  $CL = \beta_0 + \beta_1 CW$  

An orange crab gets a dummy variable of 1, thus:
$$CL = \beta_0 + \beta_1 CW + \beta_2 sp \times 1 + \varepsilon$$
becomes $CL = (\beta_0 + \beta_2) + \beta_1 CW$   

In effect, we are fitting *two* lines to this data - one for each sub-category of colour. The technical term for each of these lines is a **partial regression line**. We can write these regression lines as:

$$ shell \space length  = \beta_{0_{colour}} + \beta_{1} shell \space width + \varepsilon$$
Now the intercept parameter ($\beta_{0_{colour}}$) specifies that it is dependent on the colour of the crab as we showed above: $\beta_{0_{colour}} = \beta_0 + \beta_2$.

***

### Characters or factors?
 
 > Remember in R, categorical data is are called **factors** and the sub-groups are called **levels**

In R, both the order of the predictor variables and the order of the sub-groups of a categorical variable are relevant because this is how R partitions variation in the response variable to the predictor variables.

The jar without a lid treatment is coded as `no_lid` and the jar with a lid treatment is coded as `yes_lid`. We can also think of jar with a lid as our experimental control and the jar without a lid as our experimental treatment. 

As R shows you the **difference** between levels of a treatment and estimates these parameters based on **alphabetical ordering** of the levels, it may be helpful to think of the first level of a treatment and the first intercept and slope estimate as the control of your experiment. In other words, like a baseline you are comparing to since this is what R is doing when evaluating the one sample t test on parameter estimates.

By default, foraging strategy is classified as a **character** vector because it is a field with strings (letters). Characters do not identify sub-groups thus there is no structure to this variable - they are simply a vector of strings. Both characters and factors are ways R stores categorical data, the difference being that factors have a specific order.

Our "control" group (`yes_lid`) is alphabetically after `no_lid`, so we have the reverse scenario. Under the hood, R converts the `foraging_strategy` variable from a character vector to a **factor** when fitting the linear regression and the order of levels is assigned alphabetically.

We will not do anything about this for now - in this case, it doesn't change our conclusions. R will use `no_lid` as the baseline contrast for $\beta_0$ and $\beta_1$. We just need to keep the order in mind when parameterising.

If you needed to change the order of the sub-groups you will need to change the variable to a factor (`as.factor`) and define the order of levels using `levels = c("level1", "level2")`. If we did this, then the slope coefficient uses `yes_lid` as the baseline and the final estimated coefficients are exactly the same, just reversed.

***

### Fitting the model in `R`

A linear regression in R with two predictor variables follows the general formula: 
```
lm(Y ~ X1 + X2, data)
```
Where:

 * `X1` & `X2` are the two predictor variables
 * `+` indicates the relationship between the two predictors: a plus sign for an *additive* relationship
 * `lm` stands for linear model
 * `Y` is our response variable
 * `data` is the name of our dataset 
 * `~` indicates a relationship between our response and predictor variables

In `crabs`, our response variable is `CL` and our predictor variables are `CW` and `sp`. 

*The function below does an additive multiple regression in R but there is one mistake. Fix the error and run the code.*
```{r fixed-mod, exercise = TRUE}
lm(CL ~ CW + sp)
```
```{r fixed-mod-hint}
Are all components of the lm function included?
```

Did you get some output ?  
It should tell us two things:

1. Call is the formula used. It should be the same as the linear model code
2. Coefficients are the estimated coefficients of the model. From left to right they are: $\beta_0$, $\beta_1$ and $\beta_2$.

We can already substitute the estimated $\beta$ coefficients into the full expression of the linear model $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon$ for the crabs dataset as:

$$CL = -0.67 + 0.88 CW + 1.09 sp + \varepsilon$$

But let's look at the coefficients in more detail starting with the first two coefficients. The interpretation for these coefficients is the same as simple linear regression. There's the intercept `(Intercept)` ($\beta_0$) and there's the slope `CW` ($\beta_1$).

Remember we are expecting *two* lines in our model - one for blue crabs and one for orange crabs. **Which line are these first two coefficients referring to?**

```{r coef-cals}
quiz(
  caption = "Answer the following questions:",
  question(
    "How are these coefficients calculated?",
    answer("Ordinary least squares regression", correct = TRUE, message = random_praise()),
    answer("Ordinary most squares regression"),
    answer("Random regression"),
    answer("Random squared regression"),
    incorrect = "Incorrect. Remember back to the lecture on how linear regressions are parameterised",
    allow_retry = TRUE,
    random_answer_order = TRUE
  ),
  question_radio(
    "Given what you know about how R orders levels in a factor in alphabetical order, for which level do you think the first two coefficients are modelling?",
    answer("Blue crabs", correct = TRUE, message = "B comes before O so the first two coefficients are the intercept and slope of the linear regression for blue crabs. It also says spO in the next coefficient which tells you that is for orange crabs"),
    answer("Orange crabs"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question(
    "The value for the slope is positive. What does that mean?",
    answer("As crab shell width increases, crab shell length decreases"),
    answer(
      "As crab shell width increases, crab shell length increases",
      correct = TRUE, message = random_praise()
    ),
    answer("As crab shell width increases, crab shell length stays the same"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

***

### Parameterising our partial regression model

Because the first two coefficients are for blue crabs, we already have our equation for blue crabs with $\beta_0$ and $\beta_1$.  
Parametrised equation for blue crabs:  
*blue crab shell length* = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` *shell width*

Halfway there! Now for the orange crabs!

Since we expect the slope of the regression for orange crabs is the same as blue crabs, we already know the value of $\beta_{1}$ is `r round(coef(crabs_add)[2],1)`. But we need to manually calculate the intercept for orange crabs.

The estimate for `spO` is $\beta_2$, which is the **difference in the intercept between orange crabs and blue crabs**. To calculate the intercept for orange crabs we need to add the estimated coefficient of the intercept for blue crabs ($\beta_0$) with the difference ($\beta_2$): $\beta_{0_{colour}} = \beta_0 + \beta_2$. 

Now that you know how to parameterise the linear regression for orange crabs:  
```{r showaddcoef}
round(coef(crabs_add),1)  
```

```{r param, echo=FALSE}
coef <- round(crabs_add$coefficients, 1)
question("What is the paramterised model for orange crabs? To 1 decimal place",
         answer(paste0("CL = ", (coef[1]+coef[3]), " + ", coef[2], "CW"), correct = TRUE, message = random_praise()),
         answer(paste0("CL = ", coef[1], " - ", coef[2], "CW"), message = "Incorrect. This is the regression for blue crabs"),
         answer(paste0("CL = ", coef[3], " + ", coef[2], "CW"), message = "Incorrect. This is the difference in the intercept"),
         answer(paste0("CL = ", coef[1], " + ", coef[3], "CW"), message = "Incorrect. This is the difference in the intercept"),
         allow_retry = TRUE,
         random_answer_order = TRUE
)
```

> R shows the difference between parameter estimates so you need to calculate the correct values. They are the partial regression coefficients that show the change in the response variable with one predictor variable **while holding all other predictor variables constant.**

For example, using the mean value of orange crabs to estimate coefficients of blue crabs and vice versa, because the mean value of each of these groups represents the null hypothesis. 

In other words, if we were to accept the null hypothesis that there is no relationship between shell width and colour on shell length, then the slope of the line should be 0 and the intercept of the line should be the sample mean of shell length (ignoring shell width and colours).

***

### Predicting new values

> One application of statistical models is to make predictions about outcomes under new conditions

We can calculate the value of the response variable from any given value of the predictor variable. 

For example, we can use the parametrised equation of our model $CL = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` CW + `r round(coef(crabs_add)[3],1)` sp$ and the dummy variables 0 for Blue crabs or 1 for Orange crabs to work out the length of a crab for any value of shell width. 

If a blue crab is 10 mm wide, what is its predicted shell length?

* We are told the value of shell width (10 mm)
* We know the parameterised linear model:

$$CL = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` CW + `r round(coef(crabs_add)[3],1)` sp$$

* We can substitute the value of 10 for CW:

$$CL = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` 10 + `r round(coef(crabs_add)[3],1)` sp$$

* We know the dummy variable for blue crabs is 0 because it is the baseline level (assigned alphabetically) so we can substitute that into sp:

$$CL = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)*10` + `r round(coef(crabs_add)[3],1)` \times 0$$ 

* and solve for length:  

$$CL = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)*10`$$  

$$CL = `r round(coef(crabs_add)[1],1) + round(coef(crabs_add)[2],1)*10` mm$$ 

***

Or we can use our two partial regression models to predict the shell length of blue or orange crabs.  

*Blue crab shell length* = `r round(coef(crabs_add)[1],1)` + `r round(coef(crabs_add)[2],1)` *shell width*  
*Orange crab shell length* = `r round(coef(crabs_add)[1] + coef(crabs_add)[3],1)` + `r round(coef(crabs_add)[2],1)` *shell width*

```{r predict-fixed}
ccoef <- round(coef(crabs_add),1) 
ccoef_orange <- ccoef[1] + ccoef[3]

blue <- sample(10:70,1)
ans_blue <- round(ccoef[1] + (ccoef[2] * blue), 1)

orange <- sample(10:70,1)
ans_orange <- round(ccoef_orange + (ccoef[2] * orange), 1)

size <- sample(10:70,1)
###

quiz(caption = "Use the equation above to answer the following questions:",
  question_text(
    paste("What is the carapace length of an orange crab with a carapace width of", orange, " mm? To 1 decimal place"),
    answer(paste(ans_orange), correct = TRUE, message = random_praise()),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_text(
    paste("What is the carapace length of a blue crab with a carapace width of", blue, " mm? To 1 decimal place"),
    answer(paste(ans_blue), correct = TRUE, message = random_praise()),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_text(
    paste("How much longer is an orange crab than a blue crab with a shell width of ", size, " mm? To 1 decimal place"),
    answer(paste(round(ccoef[3],1)), correct = TRUE,
           message = "Did you do the whole calculation or notice the answer is the spO coefficient? The slopes are parallel so the answer is the difference in intercepts! The answer is the same regardless of size."),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

***

### Evaluating hypotheses

> Remember that statistical models may represent hypotheses

Here are the hypotheses:

H0: Carapace length increases with carapace width but does not differ between colour morphs  
H1: Carapace length increases with carapace width and differs between colour morphs but the rate of size increases does not vary between colours

We can test these hypotheses using the linear regression. It's important to understand these hypotheses graphically. Here are the two outcomes:
```{r hypo, fig.cap = "Two possible graphs showing linear regression lines."}
prey <- seq(0, 0.25, 0.01)
type2 <- seq(0, 1, length.out = length(prey))
par(mfrow = c(1,2))
plot(type2 ~ prey, type = 'l', lwd = 4, ylab = "Response variable", xlab = "Predictor variable",
     xaxt='n', yaxt='n', bty = "l", ylim = c(0, 1.2), col = 3, main = "Figure A")
lines(type2 + 0.2 ~ prey, lwd = 4, col = 2)
legend("topleft", bty = "n", c("Group 1", "Group 2"), lwd = c(4, 4), col = c(3,2))

plot(type2 ~ prey, type = 'l', lwd = 4, ylab = "Response variable", xlab = "Predictor variable",
     xaxt='n', yaxt='n', bty = "l", ylim = c(0, 1.2), main = "Figure B")
```

```{r lines}
question_radio(
  "Which of the graph above represents the null hypothesis?",
  answer("Figure A"),
  answer("Figure B", correct = TRUE, message = "There is no effect of the second predictor variable so the regression lines for each sub-group are the same - this is effectively a simple linear regression!"),
  allow_retry = TRUE,
  incorrect = "This describes that there is a difference between groups of the second predictor variable (colours)."
)
```

Notice how the slopes of all the lines are the same. The difference between the coloured lines in Figure A is that the lines have different intercepts. We expected this based on the additive model.

Unlike a simple regression, our hypotheses are not concerned about whether the slope is different to 0. We want to know if the lines have different *intercepts to each other*. Phrased differently, we need to test whether **the difference in the intercept is different to 0**.

We don't just need to know the difference, we need to statistically test this; whether an estimated/observed value of a sample is significantly different to a known population value. You've done this kind of test already.

```{r tests}
quiz(
  question("Can you remember which coefficient in the additive model will test this hypothesis?",
         answer("$\\beta_0$"),
         answer("$\\beta_1$"), 
         answer("$\\beta_2$", correct = TRUE, message = "spO is the difference in the intercepts"), 
         answer("$\\beta_3$"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = "We want to know whether difference in the intercepts between the groups is not equal to 0"
),
question("Which of these statistical tests you've already learnt in this module would test whether our observed slope is significantly different to 0?",
                  answer("One sample t test", correct = TRUE, message = random_praise()),
                  answer("Analysis of Variance"),
                  answer("Two sample t test"),
                  answer("Paired t test"),
                  allow_retry = TRUE,
                  random_answer_order = TRUE,
         incorrect = random_encouragement()
))
```

> Statistical significance is not the same thing as biological significance. A relationship between two purely randomly generated numbers can be statstically significant but have no biological meaning!  
Language matters when presenting results.

`R` automatically tests the following hypotheses as part of `lm`:

H0: The difference is equal to 0  
H1: The difference is not equal to 0

To see more information about our linear regression we need to ask to see the `summary` of our linear regression by placing our `lm` function within `summary(lm())`.

*Enter the code to check the `summary()` of the additive linear regression for the `crabs` dataset:*
```{r fixed-summary, exercise = TRUE}

```
```{r fixed-summary-hint-1}
Check the summary of the model with summary(). What goes inside summary()?
```
```{r fixed-summary-hint-2}
What goes inside lm()?
```
```{r fixed-summary-hint-3}
Have you included both colour (sp) and shell width (CW) to your model formula?
```

When you run `summary` you get a lot of information. Let's break it down from top to bottom:

* Call is the formula used to do the regression
* Residuals are the residuals of the ordinary least squares regression
* Coefficients are the estimated coefficients we saw earlier *plus* the standard error of these estimates, a t-value from a **one sample t-test** testing whether the estimated coefficient is significantly different to 0 and the P value of this t-test
* Some additional information about the regression at the bottom which we can ignore for now

Look at the information for $\beta_2$: the difference in the intercepts. 

```{r fixed-test}
quiz(caption = "Answer the following questions based on the R output:",
     question_text("What is the t statistic on $\\beta_2$?",
                   answer(paste(16.045), correct = TRUE),
     allow_retry = TRUE,
     incorrect = "Don't round your answer."),
     question_radio(
       "Would you accept or reject the null hypothesis of the one sample t-test on $\\beta_2$?",
       answer("Accept"),
       answer("Reject", correct = TRUE, message = "The P value is less than 0.05"),
       allow_retry = TRUE,
       incorrect = random_encouragement()
     ),
     question_radio(
       "Which statement matches your conclusion about the one sample t-test?",
       answer("The intercepts for both colour morph are the same"),
       answer("The regression lines for each colour morphs have different intercepts", correct = TRUE, message = random_praise()),
       allow_retry = TRUE,
       incorrect = random_encouragement()
     ),
     question_radio(
       "Thus, would you accept or reject our null hypothesis of the `crabs` dataset?",
       answer("Accept"),
       answer("Reject", correct = TRUE, message = "Since the regression lines are differrent, we have a graph that looks like Figure A above and there is an effect of colour on shell length that is independent of shell width.")
)
```

***

### The ANOVA table

ANOVA tables show the Sum of Squares of the linear model. This is the variation partitioned to each model coefficient.

> Variance of predictors (SS) is partitioned out from total SS in the order it is entered in to R

We can look at the ANOVA table for an linear model with `anova(lm())`. We expect to see a row for each term in the model, plus the residual variation, $\varepsilon$. From all the Sum of Squares, we can calculate the total variation in the data (SSY) and calculate metrics like the coefficient of determination $R^2$.

*Complete the code to get the ANOVA table of the `crabs` additive model*:
```{r add-anova, exercise = TRUE}
lm(CL ~ CW + sp, crabs)
```
```{r add-anova-hint}
What is the function to get the ANOVA table and where does it go?
```

Let's go through each column in the R output:

 1. The first column are all the sources of variation in shell length: shell width and colour morph
 2. `Df` stands for Degrees of Freedom (df) - this is a measure of how much information R had to fit the model. For example, the degrees of freedom of $\varepsilon$ is the total number of observations - the number of terms - 1. Thus 1 + 1 + 197 + 1 = 200 observations in the dataset.
 3. `Sum Sq` are the Sum of Squares (SS), $\sum \varepsilon ^2$, from the ordinary least squares regression - the goal is to make these as small as possible
 4. `Mean Sq` are the Mean Sum of Squares (MS). Since SS depend on the complexity of the model as described by the degrees of freedom, it's useful to standardise SS by df. This is effectively the average variation (like the sum of the error divided by the number of observations).
 5. `F value` is the F statistic. Also called the F ratio because it is the ratio between the MS of each term and the MS of the residuals
 6. `Pr(>F)` is the P value of the F statistic. The asterisks help you categorise the P values. This is another way to test whether a predictor variable or term explains a significant amount of variation in the response variable.
     * If significant, the model term explains a significant amount of variation
     * If not significant, the model term does not explain a significant amount of variation

There are three rows describing the variation in shell length: 

 1. Variation attributed to shell width
 2. Variation attributed to colour morph
 3. Variation not attributed to either shell width or colour morph - the left over variation
 
The total amount of variation in the data (SSY) is the sum of all these rows in the column `Sum Sq`.

```{r anova-quiz}
quiz(caption = "Based on the ANOVA table for the additive model on the crabs data:",
     question_text(
       paste("What is the total amount of varation in the data?"),
       answer(paste(9985.2+56.7+43.4), correct = TRUE, message = random_praise()),
       allow_retry = TRUE,
       incorrect = "SSY is the sum of all SS."
     ),
     question_text(
       paste("What is the F statistic of shell length?"),
       answer(paste(45308.97), correct = TRUE, message = random_praise()),
       allow_retry = TRUE,
       incorrect = random_encouragement()
     ),
     question("How are Mean Sum of Squares calculated?",
              answer("MS = SS/df", correct = TRUE, message = random_praise()),
              answer("MS = SS * df"),
              answer("MS = df * SS"),
              answer("MS = df/SS"),
              allow_retry = TRUE,
              random_answer_order = TRUE,
              incorrect = random_encouragement()
     ),
     question_radio(
       "Based on the F-tests in the ANOVA table, would you accept or reject our null hypothesis about crabs?",
       answer("Accept"),
       answer("Reject", correct = TRUE, message = "If H0 was supported, then the P value of sp would not be significant."),
       allow_retry = TRUE,
       incorrect = random_encouragement()
     )
)
```

***

### Putting results into sentences

 > We must always place our statsitical analysis into the wider context of our hypotheses and aims. 
 
Communicating our results is important. 

Statistical analyses need to be written into sentences, within a results section. Screenshots of R output are not appropriate because the output has no meaning to someone else. ANOVA tables should be nicely formatted in a proper table with the correct column names.

A results sentence needs at a minimum:

 * The main result
 * The name of the statistical test
 * The test statistic and degrees of freedom
     * Df can be written as a subscript to the test statistic (e.g. $t_{14}$) or reported as df = 14.
     * F statistic need the degrees of freedom for the within & among error. E.g. $F_{1,25}$
 * The P value
     * Really small or large P values can be summarised. E.g. P < 0.001
 
P values should never be reported on their own - they are also meaningless without the test statistic and context.

All the above information is given to you in `summary` or the ANOVA table. The degrees of freedom of a linear regression are found at the bottom of `summary` (`Residual standard error`).

```{r conclusion}
question("Based on the summary of the linear regression you just did, which is the most appropriate sentence reporting the results?",
         answer("There is an effect of shell width or colour on shell length (linear regression, $t_{197}$ = 16, P < 0.01)", correct = TRUE, message = random_praise()),
                  answer("There is an effect of shell width or colour on shell length (linear regression, $t$ = 16, P < 0.01)", message = "Does this include all ther information to understand the analysis?"),
         answer("There is an effect of shell width or colour on shell length ($t_{197}$ = 16, P < 0.01)", message = "Does this include all ther information to understand the analysis?"),
         answer("There is an effect of shell width or colour on shell length (P < 0.01)", message = "P values are never reported on their own"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
         )
```

Notice how the sentence does not include *significant* in the wording. Statistical significance or not is already implied by the wording of the sentence and the inclusion of the P value. The term can also be ambiguous.

Remember that biological significance is not the same as statistical significance. Biologically important patterns can be statistically non-significant and vice versa. And there are many ways that P values can be biased to give small or large values.

People's obsession over getting significant P values has driven a rise of unethical statistical practices (called P hacking or P fishing) which we do not want you as young scientists to fall into the habit of. 

 > The point of statistical testing is to understand trends in data. Not to get a significant P value. Null or non-significant results are valid results. 

***

### Practice: Parameterising a linear model of predator-prey functional response

Remember we had some unknown parameters in our predator-prey functional response model? The search rate, $a$, and the handling time, $T_h$. We need to find the value of those parameters from the slope and the intercept of our linearised Type II dataset - exactly like we did for the `crabs` example. We can use the `lm` function on our predator-prey model data to parameterise our model.

Before we can do our `lm` in `R` we need to import the data and linearise the data.

**Step 1: Importing data**  
First, we need to import the data in to R. This should be familiar to you from before. 

> * Don't use the code chunks in this tutorial to import data, it won't work. Make your own script (File -> New Script, or Ctrl(Cmd) + Shift + N). 
> * Don't write your code directly in the console either - you won't have a good record of what you've done (in case you wanted to use the code here to help you with your final report). 
> * You can copy the code in this tutorial and **modify as appropriate for your data**. 

The examples here use mathematical notation as the variable names here but the class dataset uses the names from Part 1. You should know which notation matches with what variable description.

The class dataset is provided as a comma separated values file (`.csv`). The function to import a csv file is `read.csv`:

```
class_data <- read.csv("directory/folder/class_data.csv")
```
This imports the spreadsheet into an R object called `class_data` but you can use whatever name you want. You need to replace the file address within the quotation marks with where ever you saved your file on your computer. File housekeeping is important!

> Refer to the previous drop in sessions and practicals to revise these basic steps. Make sure your address is correct and don't forget .csv at the end.

**Step 2: Linearising data**

We need to make our data linear to fit a linear model to it! Let's first take the inverse of our response variable $H_a$ to get $\frac{1}{H_a}$

```
class_data$Ha.1 <- 1/class_data$Ha
```
This line of code calculates the inverse of `Ha`, that is, 1 divided (`/`) by the number of prey captured (`Ha`), then saves that number to a new column in our dataset called `Ha.1`. Note the use of no spaces.

Your column names can be whatever is meaningful to you. E.g. you don't *have* to call your new column `Ha.1` but recognise that you need to remember to modify your code accordingly.

When manipulating data like this, it's best practice to add new columns to the data, rather than overwrite the original column. That way, if you make a mistake it's easier to see what went wrong and you won't have to start from the beginning!

> `data$column` is the general structure to select a column in R. Make sure that your dataset name and your column names in your code matches your data name and columns.

Now let's linearise the predictor variable (`H`) to get $\frac{1}{H\times T_{total}}$. 

```
class_data$HT.1 <- 1/(class_data$H * class_data$T_total)
```
Because we use a value of 1 minute for $T_{total}$, we are essentially dividing 1 by only prey density ($H$). Told you a value of 1 would make our maths easier!

Since we have done some division, it's a good time to check for any undefined values in case we divided by 0. In `R` this is denoted as infinities (`Inf`). You can check how many infinities there are using `table(is.infinite(class_data$Ha.1))` - this will check whether each cell has undefined values (`is.infinite`) and give you logical `TRUE`/`FALSE` output, then tabulate the logic statements to count the number of `TRUE`/`FALSE` occurrences in the column `Ha.1`.

We can replace the infinities with zeroes using `class_data$Ha.1 <- ifelse(class_data$Ha.1 == Inf, 0, class_data$Ha.1)`. You might recognise the if else statement and understand what's happening from previous lectures: **if** there is an undefined value, **then** replace that value with 0, **else** leave the value as is. We don't discard these observations because a value of 0 has biological meaning.  

We now have all the correct columns to parameterise our functional response.

 > **Remember how to problem solve and fix common coding problems from earlier**.

**Step 3: Doing the additive linear regression**  
If `lm` is the function to do a linear regression, `Ha.1` is the name of our response variable, `HT.1` is the name of the first predictor variable, `foraging_strategy` is the name of the second predictor variable, and the name of our dataset is `class_data`, you should be able to write the code for the linearised type II functional response (or replacing the respective components with the column name and dataset names you are using). 

(Hint: It's exactly as we did for `crabs` - so you've done it already. Remember the order of the parameters - it matters to R)

```{r error, echo = FALSE}
quiz(caption = "Getting an error message?",
     question("What gets recorded if the predator did not catch any prey?",
              answer("Ha = 0", correct = TRUE),
              answer("Nothing"),
              answer("Ha > 0"),
              answer("Ha < 0"),
              allow_retry = TRUE,
              random_answer_order = TRUE),
     question("What happens in R when you divide by 0?",
              answer("The value is undefined (Inf)", correct = TRUE),
              answer("It works"),
              answer("R crashes"),
              answer("It is the wrong function"),
              allow_retry = TRUE,
              random_answer_order = TRUE),
     question("What is the solution?",
              answer("Change the values to 0", correct = TRUE),
              answer("Delete the data"),
              answer("Collect new data"),
              answer("Use a different program"),
              allow_retry = TRUE,
              random_answer_order = TRUE,
              incorrect = "We don't throw out valid data"))
```

**Construct an additive linear model for our data.**  
Then, you should be able to identify the values for the slope and intercept and thus parameterise the linearised function in Step 4 - exactly as you did with the `crabs` dataset.

> Linear regressions are always done on the entire data, not on averages. i.e. you wouldn't use the average of your replicates for each treatment for the underlying data. To fit a line to data, ordinary least squares regression depends on quantifying *variation* of observations around the mean (think back to how sampled data from a population is distributed). Averaging data removes that variation and thus there is less information for `R` to use (fewer degrees of freedom).

We won't plot our regression lines just yet but you should be able to interpret the R output as a graph by looking at the coefficient values. We will construct a graph later!

**Step 4: Parameterising $a$ and $T_h$**  
The final step is to get our unknown values of $a$ and $T_h$ for each type of foraging strategy from our linear regression. Since we know our partial regression takes the form $Y  = \beta_{0_{foraging \space strategy}} + \beta_1 X_1$ with separate intercepts for each sub-group of `foraging_strategy` and the same slope for both groups, and we know the linearised type II model is $\frac{1}{H_a}=\ \frac{1}{a}\times\frac{1}{H\times T_{total}}+\frac{T_h}{T_{total}}$, then you should have all the information to parameterise the type II model and derive values for $a$ and $T_h$ from the linear regression output for both foraging strategies.

> Answer the CA and submit your answers. That's the end of the assessment but not the end of the practical. Keep going.

***

## Visualising data

Visualising data is very important for understanding our data and for communicating our data to others. For example, in a written report.

`plot` is the general plot function. Boxplots (`boxplot`) and bar plots (`barplot`) have their own plotting commands. You can add error bars using `arrows`.

### Theory

> The general formula to plot a scatter graph is `plot(response ~ predictor, data)`

If our data is categorised (e.g. `sp` in `crabs` has `B` or `O`), then we need to plot our points for each group separately. We can do this by calling a plot with no points using `plot(response ~ predictor, data, type = "n")` where `"n"` tells `R` not to plot anything. Then we add points manually with `points(response ~ predictor, data[data$group == "subsetA",])` for each sub-category.

We need to subset our data (like we did in previous pracs) so that `R` only plots the sub-categories of data.

*Using the crabs dataset, modify the code below to plot crab shell length (response) against crab shell width (predictor) for __only blue crabs__.*
```{r graph, exercise=TRUE, exercise.lines = 5}
plot(response ~ predictor, data, type = "n")
points(response ~ predictor, data[data$group == "subsetA",])
```
```{r graph-hint-1}
Have you replaced all the terms with the corresponding one from the crabs dataset?
```
```{r graph-hint-2}
Are you cases correct?
```
```{r graph-hint-3}
You should have two lines of code.
```
```{r graph-solution}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
```

If you are correct, your graph should match the one below:
```{r crabsol}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
```

*Now add the orange crabs to plot all the data - You have to start the code from the beginning.*
```{r orangec, exercise=TRUE, exercise.lines = 5}
plot(response ~ predictor, data, type = "n")
points(response ~ predictor, data[data$group == "subsetA",])
```
```{r orangec-hint-1}
Have you replaced all the terms with the corresponding one from the crabs dataset? You can copy your code from before so you don't have to type it out again
```
```{r orangec-hint-2}
Are you cases correct?
```
```{r orangec-hint-3}
You should have three lines of code
```
```{r orangec-solution}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
points(CL ~ CW, crabs[crabs$sp == "O",])
```

Uh oh! We cannot distinguish between the colours of crabs! Time to learn more `R` to add more features to the graph.

***

#### Colours

Colour is important when presenting data. Are the colours meaningful? Are they necessary? Can they be clearly distinguished? Are they appropriate for screens, for printing, or accessible for colour-blind people?

> Red-Green colour blindness is the most common form of colour blindness. A simple guideline is to avoid using red and green together where ever possible. Blue and orange are better constrasting colours (that's why they are common colour schemes for movie posters).

Colour in `R` is defined by `col`. So in a graph if I wanted to change the colour of the points from black (default) to red then I can either call `col = "red"` or `col = 2` as an argument within the `points()` function, because red is the second colour in the default R colour palette (black is 1). There are lots of colours to choose from (Google it for a full list). R also accepts hexidecimal RGB colour codes for custom colours (e.g. black is #000000).

*Change the colour of the points to their relevant colour (blue or orange)*
```{r col, exercise=TRUE, exercise.lines = 5, exercise.eval = TRUE}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",])
points(CL ~ CW, crabs[crabs$sp == "O",])
```
```{r col-hint-1}
col stands for colour and tells R what colour to pick. colours are in lowercase
```
```{r col-hint-2}
Have you remembered to use quotation marks? You can also use numbers in the default palette.
```
```{r col-solution}
plot(CL ~ CW, crabs, type = "n")
points(CL ~ CW, crabs[crabs$sp == "B",], col = "blue")
points(CL ~ CW, crabs[crabs$sp == "O",], col = "orange")
```

***

#### Shapes

Shapes are important because it makes the points stand out. It can also be used to distinguish between groups of data on the same graph. Sometimes it's necessary to change colour *and* shape to make it easier to distinguish between groups - redundancy is acceptable and encouraged in graphical design.

The shape of the points are coded `pch = <number>` as an argument within `plot()` or `points()`. There are lots of options designated a number from 1 to 25. (Google "R pch" for the full list). 1 is the default open circle. A filled circle is 16.

*Change the shape of the points from open to filled circles.*
```{r pch, exercise=TRUE, exercise.lines = 5, exercise.eval = TRUE}
plot(CL ~ CW, crabs, pch = 1)
```
```{r pch-hint-1}
pch tells R what shape to pick. It accepts a number
```
```{r pch-hint-2}
What is the number for filled circles?
```
```{r pch-solution}
plot(CL ~ CW, crabs, pch = 16)
```

You can combine all the code above to change the colour, shape and axes labels.

This graph is fine but if we were to put this in a professional scientific paper or report there are a few missing elements and we may want to customise the aesthetics for a prettier graph.

***

#### Axes labels

The `R` code to change the labels on a graph are `xlab` and `ylab` for the x axis and y axis, respectively. These are called within the `plot()` function like:
```
plot(response ~ predictor, data, xlab = "label", ylab = "label")
```

*Change the x axis label of our `crabs` plot to "Width" instead.*
```{r axes, exercise=TRUE, exercise.lines = 5, exercise.eval = TRUE}
plot(CL ~ CW, crabs)
```
```{r axes-hint-1}
The code to define the x axis label is xlab
```
```{r axes-hint-2}
Have you  remembered to use quotation marks?
```
```{r axes-solution}
plot(CL ~ CW, crabs, xlab = "Width")
```

In practice, we want our figures to be informative and be understandable independently of the main text. This means having informative labels and including units.

*There are __two mistakes__ in the following code to change the axis labels to "Width" or "Length" AND include units (mm) in both the x and y axis. Fix it and run the code*
```{r units, exercise=TRUE, exercise.lines = 5}
plot(CL ~ CW, crabs, ylab = "Width (mm)", ylab = "Length (cm)")
```
```{r units-hint-1}
The code to define the x axis label is xlab
```
```{r units-hint-2}
Have you remembered to use quotation marks?
```
```{r units-solution}
plot(CL ~ CW, crabs, xlab = "Width (mm)", ylab = "Length (mm)")
```

***

#### Regression lines

Once we've done the hard work to do a linear regression, it's nice to add it to our graph so we can see how it fits to the data. A linear regression is always a linear line (straight line) so we can use the code to plot a straight line.

The formula to plot a straight line is `abline(intercept, slope)` because it plots a line from a to b. The intercept is the first value, the slope is the second value. You need to plot the data first before adding additional lines.

Here are the coefficients for a simple linear model for the crabs showing the intercept and slope respectively:
```{r coef-reminder}
round(crabs_lm$coefficients, 1)
```

*Complete the `abline()` formula to plot our regression line then press run.*  
```{r abline, exercise=TRUE, exercise.lines = 5}
plot(CL ~ CW, crabs)
abline()
```
```{r abline-hint}
The slope and the intercept of our model was calculated by lm(CL ~ CW, crabs)
```
```{r abline-solution}
plot(CL ~ CW, crabs)
abline(-0.662, 0.9)
```

The same code is used to plot either regression line for a multiplicative regression - you just need to use the slope and intercept values you want!

You can also change the colour, type and width of the line as arguments within `abline()`.

* `lwd = <number>` is line width. The default is 1. Values greater than 1 are a thicker line.
* `lty = <number>` is line type. The default solid line is 1. Different types of dashed or dotted lines are numbers 2 to 6.

***

#### A final graphing test

Let's integrate everything we've leant today and plot the graph of our additive linear model with the correct regression lines.

```{r final-graph, fig.cap= "A complete graph for a scientific report"}
plot(CL ~ CW, crabs, type = "n", xlab = "Width (mm)", ylab = "Length (mm)")
points(CL ~ CW, crabs[crabs$sp == "B",], col = "blue", pch = 16)
points(CL ~ CW, crabs[crabs$sp == "O",], col = "orange", pch = 16)
abline(-0.6606, 0.8848, col = "blue")
abline((-0.6606 + 1.0910), 0.8848, col = "orange")
```

Here are the coefficients for the interactive linear model:  
```{r int-coef}
round(crabs_add$coefficients, 1) 
```

*Change the basic crabs graph to match the graph above - this graph is suitable for a scientific report*
```{r final-graph-test, exercise=TRUE, exercise.lines = 6, exercise.eval = TRUE}
plot(CL ~ CW, crabs)
```
```{r final-graph-test-hint-1}
You need to change the axes labels
```
```{r final-graph-test-hint-2}
You need to plot the points for different coloured crabs separately on a blank graph
```
```{r final-graph-test-hint-3}
You need to change the shape type
```
```{r final-graph-test-hint-4}
You need to change the point colours
```
```{r final-graph-test-hint-5}
You need to add the regression lines separately, including the aesthetics
```

#### Other variables in `plot`

**Borders**   
A border around the plot is plotted by default. This is dictated by the default `bty = "o"`. You can remove the border entirely with `bty = "n"` and you can plot only the bottom and right border (axes) with `bty = "l"` - that's a lowercase L, not a number 1. Borders also apply to legends.

**Legends**  
Figure legends can be added using `legend`. Check out the help file for details because you need to set:

 * Where to put the legend
 * The text of the legend
 * The colours
 * The lines or points used
 
**Title**  
Figure titles can be set using `main = "<title>"`. There are other ways of captioning figures too, like subtitles.

**Lines**  
`type` tells R what kind of plot you want. R plots points (`type = "p"`) by default but can also plot lines (`type = "l"`), [lowercase L]. You can plot a combination with `type = "b`.

`lines` is the equivalent to `points` for plotting individual lines.

***

#### Captions

The last thing every scientific graph needs is a caption. The caption needs to describe the graph and explain every aspect of the graph to the reader *independently* of the main text.

* What is the overall relationship?
* What is on the x axis? What is on the y axis?
* What is the sample size?
* What do the colours, lines or points represent?

Often a single sentence saying "Figure 1. A graph of the response against the predictor" is not enough information in a professional report.

***

### Practice: Graphing results with a caption

Use your new found knowledge of making graphs in R to make a plot of our results from our predator-prey interaction response including appropriate axes labels with units. The aesthetics are up to you. Write a caption as well.

You can show it to your demonstrator for feedback.

***


##

That's the end. Take a break. Stand up. Shake your limbs. Breathe.

```{r break-quiz}
question(
  "Who lives in a pineapple under the sea?",
  answer("Haven't you asked this already?", correct = TRUE),
  answer("Patrick Star"),
  answer("Squidward Tentacles"),
  answer("Sandy Cheeks"),
  random_answer_order = TRUE
)
```

> Make sure you have submitted your answers

***
## The $\beta_3$ coefficient

The difference between an additive and interactive model is the inclusion of the coefficient $\beta_3$ to derive the full model:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$$

The interpretation is **exactly** the same as the additive model. The coefficients $\beta_0$, $\beta_1$ and $\beta_2$ mean the same thing as before. 

> $\beta_3$ is how $X_2$ affects the relationship between $Y$ and $X_1$

Graphically, this is how $X_2$ affects the *slopes* of the lines.

In an additive model we had two lines with the same slope. In multiplicative models, both the slope and the intercept are estimated for each level of the factor. The two lines have different slopes *and* intercepts.

We won't fit an interactive model to our data but we will explore fitting interactive models with the `crabs` dataset because it's something we need to know how to do.

We can write the full model in the partial equation form as:

$$ shell \space length = \beta_{0_{colour}} + \beta_{1_{colour}} shell \space width + \varepsilon$$
Now, both $\beta_0$ and $\beta_1$ vary according to colour, where $\beta_{1_{colour}} = \beta_1 + \beta_3$: the slope of the line and how the slope of the line changes with sub-groups of colour.

```{r quiz3}
question_text(
  paste("How many partial regression lines are fitted to our crab dataset when we do a multiplicative model?"),
  answer("2", correct = TRUE, message = "Two lines. One for orange crabs and one for blue crabs. Each with different slopes and intercepts"),
  allow_retry = TRUE,
  incorrect = "Incorrect. Think about what the mathematical linear model is saying and try again"
)
```

***

### Linear models with interactions in R

> The structure of the `lm()` function for an interactive model is the same as the additive model but now the relationship between the predictor variables is denoted by `*` to indicate the interaction instead of `+`. 

It's time to do the linear model.

*Change this additive linear model function into a multiplicative linear regression. Press run.*
```{r random, exercise = TRUE, exercise.lines = 5}
lm(CL ~ CW + sp, crabs)
```
```{r random-hint}
What's the difference in the R formula between an additive model and a multiplicative model?
```
```{r random-solution}
lm(CL ~ CW * sp, crabs)
```

As expected, we see that there are **four** coefficients. From left to right they are $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$ in the full model $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon$. These represent the baseline intercept and slope, and the difference in the intercept and slope, respectively.

***

### Parameterising the model

The process of parameterising the model is *exactly* the same as the additive model so you should be expecting what's coming up. 

> In R `:` means "interaction" so `X1:X2` means this is the interaction between `X1` and `X2`

```{r param-quiz, echo=FALSE}
crabs_int
question("What do the estimated coefficients for spO and CW:spO represent?",
         answer("The difference in the slope or intercept for orange crabs compared to blue crabs", correct = TRUE, message = random_praise()),
         answer("The difference in the slope or intercept for blue crabs compared to orange crabs", message = "Incorrect. Remember R is calculating blue crabs first before orange crabs because they are in alphabetical order"), 
         answer("The estimated slope or intercept for orange crabs"), 
         answer("The estimated slope or intercept for blue crabs"),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```

In the additive model we only needed to manually calculate the intercept by adding $\beta_0$ and $\beta_2$. Now, we **also** need to manually calculate the slope by adding $\beta_1$ and $\beta_3$.  


> The coefficient estimate `CW:spO` is the **difference in the value of the slope for orange crabs relative to the slope for blue crabs** `CW`. This is $\beta_3$.

Remember: blue is alphabetically before orange so R calculates the variances for blue crabs first, then orange crabs.  

We are given the slope for blue crabs already. We need to work out the slope for orange crabs by adding the coefficient estimates together.

What we need:  
Parametrised slope for orange crabs = Slope for blue crabs $\beta_1$ + difference in slope for orange crabs compared to blue crabs $\beta_3$  

Information provided from the `R` output:  
Slope for blue crabs = `r round(coef(crabs_int)[2],2)`  
Difference in slope for orange crabs compared to blue crabs = `r round(coef(crabs_int)[4],2)`

```{r orange-slope}
question_text(
  paste("Use the above information to calculate the slope for orange crabs"),
  answer(paste(round(coef(crabs_int)[2],2) + round(coef(crabs_int)[4],2)), correct = TRUE, message = random_praise()),
  allow_retry = TRUE,
  incorrect = "Keep trying - you have all the information already"
)
```

***

### The final parameterised models

Did you get the parameterised partial regression equations below?  

*blue crab shell length* = `r round(coef(crabs_int)[1],2)` + `r round(coef(crabs_int)[2],2)` *shell width*  
*orange crab shell length* = `r round(coef(crabs_int)[1] + coef(crabs_int)[3],2)` + `r round(coef(crabs_int)[2] + coef(crabs_int)[4],2)` *shell width*

The good news is that was as hard as it gets in this module.  

 > Parameterising the equation is simply knowing what the coefficients represent and knowing which ones to add together

***

### Evaluating hypotheses

The `summary` of the linear model is the same as the additive model.

*Complete the code to get the `summary()` of our multiplicative model*
```{r summary-random, exercise = TRUE, exercise.lines = 5}
lm(CL ~ CW * sp, crabs)
```

> H0: There is no effect of shell width or colour or their interaction on shell length  
> H1: There is an effect of shell width or colour or their interaction on shell length

Like with the additive model, the hypothesis tests on the coefficient estimates for the slope are the important ones:

* `CW` - This tests whether the slope of blue crabs is different to 0
    * This gives an indication of whether shell width can predict shell length
* `CW:spO` - This tests whether the slope for orange crabs is different to blue crabs
    * This tests whether there is an interaction between colour and shell width
    * If this is non-significant, then a multiplicative model is overly complex to describe our biological pattern (because it uses too many parameters) and perhaps an additive model is a better descriptor. Aiming to minimise the number of parameters used in a linear model is the statistical concept of **model parsimony**. 


```{r random-test}
quiz(caption = "Answer the following questions:",
  question_radio(
    "Based on the test on the slope coefficient above, would you accept or reject the null hypothesis?",
    answer("Accept"),
    answer("Reject", correct = TRUE, message = "The P value is less than 0.05"),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  ),
  question_checkbox(
    "What does the multiplicative linear regression tell you about the relationship between our variables? Select all that apply",
    answer("There is no interaction between shell width or colour on shell length"),
    answer("There is an interaction between shell width or colour on shell length", correct = TRUE),
    answer("The relationship between shell length and shell width is different between blue and orange crabs", correct = TRUE),
    answer("The effect of shell width on shell length is dependent on the colour of the crab", correct = TRUE),
    allow_retry = TRUE,
    incorrect = random_encouragement()
  )
)
```

***

### The multiple regression ANOVA table

Here is the ANOVA table for the interactive linear model:

```{r anova-table-random}
fixp <- function(x, dig=3){
  x <- as.data.frame(x)
  
  if(substr(names(x)[ncol(x)],1,2) != "Pr")
    warning("The name of the last column didn't start with Pr. This may indicate that p-values weren't in the last row, and thus, that this function is inappropriate.")
  x[,ncol(x)] <- round(x[,ncol(x)], dig)
  for(i in 1:nrow(x)){
    x[i,ncol(x)] <- ifelse(x[i,ncol(x)] == 0,
                           paste0("< 0.", paste0(rep(0,dig-1), collapse=""), "1"),
                           x[i,ncol(x)])
  }
  x
}
knitr::kable(fixp(anova(crabs_int)), digits = 3)
```

This is what the above ANOVA table is showing:  

|  Source of variation  | SS | df | MS | F | P |
|:---------------------:|:--:|:--:|:--:|:-:|---|
| Factor X1 | SSR of X1 | number of levels of X1 - 1 | |  |   |
| Factor X2 | SSR of X2 | number of levels of X2 - 1 ||   |   |
| Factor X1 x X2 | SSR of X1 & X2 | df of X1 x df of X2 ||   |   |
| Within error | SSE | levels of X1 x levels of X2 X (number of observations - 1) | |   |   |
|  Total error   | SSY | (levels of X1 x levels of X2 X number of observations) - 1 |    |   |   |

We can also use the F test on the interaction in the ANOVA table to test the null hypothesis that the effect of shell width on shell length is not dependent of the colour of the crab.

```{r quiz4}
question_radio(
  "Based on the F test on the interaction in the above ANOVA table, would you accept or reject the null hypothesis?",
  answer("Reject", correct = TRUE, message = "The P value is less than 0.05. There is an interaction between colour and shell width, or the effect of shell width on shell length is dependent of the colour of the crab"),
  answer("Accept"),
  allow_retry = TRUE,
  incorrect = random_encouragement()
)
```

***

## End

That's the end of the prac. We've covered a lot of practical skills that you can use in your final report, in the future and in your other modules. Just as you should build upon what you've learnt in previous modules in this one.

Here's a summary of what we've done:

 * Parametrise a linear regression in R with biological data
 * Made predictions with linear models
 * Interpret linear regression output to test hypotheses
 * Write a results paragraph
 * Make a graph in R with appropriate labels and caption
 
Over the course of the last practical and this one, we have done a crash course in the Scientific Method from formulating a hypothesis from a biological system (infection or predator/prey response) to evaluating hypotheses and every thing in between.

One last summary quiz:
```{r coef-quiz, echo=FALSE}
question("Which coefficient is present in a full model that is not present in an additive model?",
         answer("$\\beta_0$"),
         answer("$\\beta_1$"), 
         answer("$\\beta_2$"), 
         answer("$\\beta_3$", correct = TRUE, message = random_praise()),
         allow_retry = TRUE,
         random_answer_order = TRUE,
         incorrect = random_encouragement()
)
```
