---
title: "Statistical modelling 1: Simple linear models"
author: |
 [Dr Jacinta Kong](https://jacintak.github.io/index.html)
 | School of Natural Sciences
 | kongj@tcd.ie
subtitle: |
  | [Back to jacintak/biostats](https://jacintak.github.io/biostats/index.html)
editor_options:
  chunk_output_type: console
output: 
  html_document:
    toc: true # table of contents
    toc_float: true
    theme: readable
    code_download: yes
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = F, warning = FALSE, message = FALSE, comment = NA, fig.height = 3, fig.width = 3, dpi = 100, fig.align = "center", fig.show = "hold")
library(tidyverse)
theme_set(theme_classic())
library(MASS)
```
***

# Overall lecture aims

* Identify the structure of general linear models
* Describe how linear models are parametrized
* Understand how to use linear models
* Understand how to interpret and evaluate linear models

Recommended reading:
Chapter 5.5 Getting Started with R


## Other resources
 
* Seeing Theory. Chapter 6. https://seeing-theory.brown.edu/regression-analysis/index.html
* Statistics: an introduction using R. Michael J. Crawley. Wiley Press. Chapters 7 - 11.
* Experimental Design and Data Analysis for Biologists. Gerry P. Quinn & Michael J. Keough. Cambridge Press. Chapters 5, 6, 8, 12.
* R for Data Science (For more advanced R). https://r4ds.had.co.nz/index.html
* https://learningstatisticswithr.com/book/index.html

## By the end of this lecture you should:
 
* Describe the structure of a general linear model
* Understand how a simple linear model is parametrised
* Understand what the parameters of a linear model represent
* Parametrise a simple linear model from given information
* Interpret a simple linear model in a graphical format

***

# Height and girth of trees

Imagine we measured the height and girth of trees to test the hypothesis that thicker trees are taller.  

```{r linear model}
tree_plot <- ggplot(trees, aes(Girth, Height)) + geom_point()
tree_plot
```

How should test our hypothesis? We could assign trees to categories like "thick" or "thin" trees and do a t-test. What does our t-test say?

```{r t test, fig.show='asis'}
trees %>% 
  mutate(Thickness = ifelse(Girth <= median(Girth), "Thin", "Thick")) %>% 
  ggplot(aes(Thickness, Height)) + 
  stat_summary(fun.y = mean, geom = "bar", fill = 'grey') +
  stat_summary(fun.data = "mean_se", geom = "errorbar", lwd = 1, width = 0.3)
t.test(Height ~ Thickness, trees %>% 
  mutate(Thickness = ifelse(Girth <= median(Girth), "Thin", "Thick")))
```

## **But** this isn't the best way

* We lost information using groups - tree girth is not categorical data, it's continuous
* This increases unexplained variance = *bad!*
* When might you want to group data?

***

# Linear regression

> models the linear relationship between 2 continuous variables

* Continuous response variable (Y)
* Continuous predictor variable (X)

```{r regression model, fig.cap= "Fit a line"}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2)
```

## What does linear mean?

* By definition, all predictor parameters should not be divided by each other
* No parameter is an exponent
* No parameter is multiplied or divided by another

Not a linear model:  
$H_a = \frac{a \times H \times T}{1 + a \times H \times T}$ e.g. Hollingâ€™s Type II predation equation  
$y = \frac{\beta_1 x}{\beta_2 x}$ e.g. enzyme rate of reactions

## Why do we use a linear model?

* Quantify a relationship
  - E.g. relationship between drug concentration and time
* Predict what will happen
  - E.g. growth at different time points
* Explain as much about the response variable as possible
* Partition variation
  - E.g. Phenotypic variation = genetic variation + environmental variation

***

# Structure of a general linear model

> $$Y = intercept + slope \times X$$

```{r general linear model}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2) +
  xlab("Predictor variable, X") +
  ylab("Response variable, Y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank()) 
```

Intercepts and slopes can be mathematically represented by any symbol or letter.

Mathematically, the equation is written as:
$$Y_i = \beta_0 + \beta_1 \times X_i + \varepsilon_i$$
Where $i$ refers to individual data points.

## Variables of a linear model

Variables are $y_i$ and $x_i$  
$y_i$ = response variable, what you are interested in, e.g. height  
$x_i$ = predictor variable, what you are manipulating, e.g. girth


## Parameters of a linear model

Parameters are what we don't know and we need to work out (*parametrise*). They are $\beta_0$ and $\beta_1$.   
$\beta_0$ = population intercept, value of y when x is 0, constant  
$\beta_1$ = population slope, unit change in y with a unit change in x  
$\varepsilon_i$ = the residual of the model (how much the line gets wrong for each data point or what is left over), also called **random error**  

***

# Ordinary least squares regression

Ordinary least squares regression (OLS) is the technique used to parametrise the linear model by finding the 'best fitting' line.

> The aim is to make the sum of the squared residuals, $\sum{\varepsilon_i^2}$, as small as possible 

Residuals are squared to make all values positive - remember points can fall above and below the best fit line.

```{r residuals, fig.cap = "Residuals (blue lines) are the difference between the data point and the predicted line (black line)",fig.width= 5, fig.height=5,}
plot(Height ~ Girth, trees)
tree_lm <- lm(Height ~ Girth, trees)
abline(tree_lm, lwd=3)
for(i in seq_along(trees$Height)) {
	  yval <- coef(tree_lm)[1]+coef(tree_lm)[2]*trees$Height[i]
	  segments(x0 = trees$Girth[i], y0 = trees$Height[i], 
	           x1 = trees$Girth[i], y1= coef(tree_lm)[1]+coef(tree_lm)[2]*trees$Girth[i], col = "blue")
	}
```


## Finding the intercept

* Why not extrapolate to 0?
* We know $\bar{y}$ and $\bar{x}$ - the means of x and y
* We can rearrange the linear equation to $\beta_0 = \bar{y} - \beta_1 \bar{x}$ where $\beta_1$ is the estimated slope

***

# What does a linear model tell us?

> 1. What will Y be, given a new value of X?
> 2. Does the population slope $\beta_1$ differ to 0?

## Back to trees

The function `lm()` in `R` stands for linear model. It will do the OLS calculations for us. The function takes data in the form `lm(Y ~ X, data)`. Use `summary()` on your `lm` to get more information.

```{r trees lm}
tree_lm
```

Here the `(Intercept)` is the value of the intercept, $\beta_0$ and `Girth` is the slope of the model, $\beta_1$.  
We then put these numbers in our linear model equation to get the **parametrised linear model**

$Y = intercept + slope \times X$  
becomes $Height =$ `r round(coef(tree_lm)[1],3)`  + `r round(coef(tree_lm)[2],3)` $\times Girth$

```{r tree plot 2, fig.cap = "Does the above equation match this line?"}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2)
```

## Calculating a new value of Y

We can use the parametrised equation to work out the height of a tree for any value of girth. If a tree is 10cm thick, what is its predicted height?

$Height =$ `r round(coef(tree_lm)[1],3)`  + `r round(coef(tree_lm)[2],3)` $\times Girth$

$Height =$ `r round(coef(tree_lm)[1],3)`  + `r round(coef(tree_lm)[2],3)` $\times 10$  
$Height =$ `r round(coef(tree_lm)[1],3) + 10 * round(coef(tree_lm)[2],3)` cm

## Does the population slope $\beta_1$ differ to 0?

Typically we are more interested in the value of the slope than the intercept. We want to know if the slope of the fitted line is statistically different to 0 because that represents our hypotheses:

> Null hypothesis (in orange):
> $$H_0: \beta_1 = 0$$
> Alternative hypothesis (in blue):
> $$H_1: \beta_1 \neq 0$$

```{r tree slopes, fig.cap = "Hypotheses of linear models"}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2) +
  geom_hline(yintercept = mean(trees$Height), colour = "orange", lwd = 2)
```

```{r regression slopes, fig.cap = "Regression lines can have positive (blue) or negative (red) slopes, either are H1"}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2) +
  geom_abline(slope = -1.054, intercept = 90, lwd = 2, colour = "red")
```

### Testing linear model parameters in R

R will conduct a statistical test on the model parameters for us. You can see it using `summary(lm())`

```{r model output}
summary(tree_lm)
```

* The P values above test whether our slope is significantly different to 0 
* Tested like a single parameter t-test (why?)
  - What are the degrees of freedom?
  
**Based on the R output above, does our estimate slope for Girth differ to 0 and what can we conclude about our hypothesis?**
