---
title: "Checking assumptions"
output: learnr::tutorial
runtime: shiny_prerendered
description: > 
    Let's check our assumptions about linear regression
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
```

# Checking assumptions

Checking that our data meets the assumption of linear models is pretty important. Else, we might make the wrong conclusion! Unfortunately, these checks may be ignored or forgotten about but that's no excuse.  

Here are some things to look out for:

## Normality {data-progressive=TRUE}

Normality can be checked using Quantile-Quantile plots.  
Quantile-Quantile (Q-Q) plots are useful alternatives to visualising distributions to density plots or histograms. They are easier to assess distribution and normality with than histograms.

### Q-Q plot of single variables

We can see whether a single variable has a normal distribution - specifically that the distribution is symmetrical or not skewed - using the function `qqnorm(data$variable)`.

```{r chichwk, echo=TRUE}
qqnorm(chickwts$weight) # the weight of chicks fed different diets (built in dataset)
```

Here, the quantiles of our observations are plotted against the theoretical quantiles if our observations followed a normal distribution.  

**Question** 
```{r quiz1}
question_radio(
  "What would you expect to see if our observations followed a normal distribution?",
  answer("A random scatter of observations"),
  answer("Observations to fall along a straight line at approx a 45 degree angle", correct = TRUE),
  allow_retry = TRUE
)
```

We can add a reference line to the above to help us evaluate how linear the Q-Q plot is. We can make the line red and thicker for fun.

> The thickness of a line in an R plot is changed using `lwd` (line width) & the colour of something in a graph is changed using `col`

Here is the code to plot our Q-Q plot and a reference line. Change the `qqline` function so that we can have a red line that is 2 units thick for fun.

```{r qqref, exercise=TRUE, exercise.lines = 3}
qqnorm(chickwts$weight) # the weight of chicks fed different diets (built in dataset)
qqline(chickwts$weight, col = "hotpink", lwd = 0.1)
```
```{r qqref-hint}
Which two parameters do we need to change to change colour and line thickness?
```
```{r qqref-solution}
qqnorm(chickwts$weight) # the weight of chicks fed different diets (built in dataset)
qqline(chickwts$weight, col = "red", lwd = 2)
```

Do you think the observations follow a normal distribution?  

Compare the above with the histogram and density plots

```{r compare, echo = TRUE}
par(mfrow = c(1,2)) # plot two graphs side by side. horizontally (two columns, one row)
plot(density(chickwts$weight), col = "purple") # density plot, purple for fun
hist(chickwts$weight, col = "yellow") # histogram, yellow for fun
```

**Question** 
```{r quiz2}
question_radio(
  "From all your lines of evidence, would you conclude that the distribution of chick weights is normal?",
  answer("Yes", correct = TRUE, message = "It looks ok - real data is rarely perfectly normal"),
  answer("No"),
  allow_retry = TRUE
)
```


For comparison look at the Q-Q plots of a gamma distribution (distinctly not normal)

```{r gamma, echo = TRUE}
par(mfrow = c(1,2)) # plot two graphs side by side. horizontally (two columns, one row)
plot(density(rgamma(100, 3, 5)))
qqnorm(rgamma(100, 3, 5))
qqline(rgamma(100, 3, 5)) # You should be able to see the skewness in the data. Compare with hist()
```

### What happens with data that is not continuous? 

Q-Q plots also work with visualising data that is not continuous.

```{r discrete, echo = TRUE}
qqnorm(Loblolly$age) # the ages of pine trees, can also try rbinom(100, 10, 0.5)
qqline(Loblolly$age)
```

**Question** 
```{r quiz3}
question_radio(
  "What is the main difference between the Q-Q plots of the continuous and discrete data?",
  answer("They are the same"),
  answer("You can see a staircase pattern", correct = TRUE, message = "The clustered groupings in the Q-Q plot show the discrete nature of the observations"),
  allow_retry = TRUE
)
``` 

You can also see it using `hist()`

**Test yourself**
```{r quiz4}
question_radio(
  "Would you expect to see a staircase pattern in a Q-Q plot of the number of cases of esophageal cancer in a population?",
  answer("Yes", correct = TRUE, message = "The number of cases - a count - is a discrete variable"),
  answer("No")
)
``` 


### Q-Q plots of residuals for assessing normality

Q-Q plots permit comparison of two probability distributions when one distribution is the expected and the other is the observed distribution, then we can evaluate how well our observations follow our expected distribution. Using Q-Q plots we can assess skewness or identify outliers or influential points.

Q-Q plots are automatically generated when calling plot on a linear model (`lm`). It's the second graph called (defined using which). You can also make one using `qqplot()`

```{r normal, echo = TRUE, fig.cap="Does that look normal to you?"}
plot(lm(Height ~ Girth, trees), which = 2)
```

### Another use of Q-Q plots

We can also compare the distribution of two variables. If they are distributed equally then they should fall along the straight line.

```{r two}
qqplot(trees$Height, trees$Girth)
```

Compare with:

```{r two2, fig.height=8}
par(mfrow = c(2,1))
hist(trees$Height)
hist(trees$Girth)
```

> We can deal with non-normal data by applying a transformation (e.g. log10), collecting more data  (if a low sample size is the root cause of non-normality) or making sure that a linear regression following a normal distribution is the most appropriate way to analyse our data

***

## Homoscedasticity

Homoscedasticity is the statistical term for homogeneity of variance. The opposite is called *Heteroscedasticity*. 

If "homo" means the same & variance is the spread of observations around the mean, what do you think the definition of homoscedasticity is?

**Question** 
```{r quiz5}
question(
  "What would you expect to see in a bar plot showing means and standard deviation for the assumption of homogeneity of variance to be met?",
  answer("The error bars of standard deviation to be similar", correct = TRUE, message = "We want the variation across our grouped observations to be similar so the error bars of standard deviation should also be similar"),
  answer("The error bars of standard deviation to be different"),
  answer("The error bars of standard deviation to be small"),
  answer("The error bars of standard deviation to be large"),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
```

```{r homo, echo=FALSE, fig.cap = "Bar plot of mean of two groups (A & B). Error bars indicate standard deviation"}
data <- rbind(data.frame(y = rnorm(50, mean = 50, sd = 5), x = rep("A", 50)),
              data.frame(y = rnorm(50, mean = 50, sd = 20), x = rep("B", 50)))
summ_data <- rbind(tapply(data$y, data$x, mean),
                   tapply(data$y, data$x, mean)-tapply(data$y, data$x, sd),
                   tapply(data$y, data$x, mean)+tapply(data$y, data$x, sd))
bplot <- barplot(summ_data[1,], ylim = c(0, 75))
arrows(bplot,summ_data[3,], bplot, summ_data[2,], angle=90, code=3, length=0.1) 

question_radio(
  "Look at the above graph - does it show homoscedasticity or heteroscedasticity?",
  answer("Heteroscedasticity", correct = TRUE, message = "Here, you can see that group `B` has a higher standard deviation than group `A`. So, we would conclude that there is heteroscedasticity. We do not want heteroscedasticity because it would bias the calculations of variance if we were to do an ANOVA. Remember that ANOVA is a test of variance"),
  answer("Homoscedasticity")
)
```

The same concept applies for scatter plots. 

```{r scatter, echo=FALSE, fig.cap= "A scatter plot and a fitted model", warning=FALSE}
data <- data.frame(x = seq(1,100))
for(i in seq_along(data$x)){
  data$y[i] <- data$x + (20 + seq(0,20, length.out = length(data$x))[i]) + rnorm(1, 0, seq(1,15, length.out = length(data$x))[i])
}
plot(y ~ x, data, ylim = c(0, 60))
abline(lm(y~x, data))

question_radio(
  "Look at the above scatterplot - does it show homoscedasticity or heteroscedasticity?",
  answer("Homoscedasticity"),
  answer("Heteroscedasticity", correct = TRUE, message = "Here, you can see that as the value of `x` increases so does the scatter around the trend line in `y`. It is sometimes referred to as a shotgun pattern or cone/triangle pattern")
)
```

Heteroscedasticity is common in time series data because your observations are not independent of each other, the value of one observation is dependent on what happens earlier in time. In other words, your response variable can be modelled based on the standard deviation.

**Question**
```{r scatter2}
question(
  "What would you expect to see in a scatter plot for the assumption of homogeneity of variance to be met?",
  answer("Observations to distribute evenly along a trend line", correct = TRUE, message = "We want the variation across our grouped observations to be similar so the spread of observations along they axis (vertically) across the values of x (horizontally) should be similar."),
  answer("Observations to distribute unevenly along a trend line"),
  answer("Observations to be further from a trend line at low values of X"),
  answer("Observations to closer to a trend line at low values of X"),
  allow_retry = TRUE,
  random_answer_order = TRUE
)

```

You can assess this for a linear model from the (standardised or non-standardised) residual plot vs fitted values. Here's the residual plot using the above data. Can you see the unequal variance?

```{r resid, echo=FALSE, results = 'hold'}
plot(lm(y~x, data), which = 1)
plot(lm(y~x, data), which = 3)
```

> One way of dealing with heteroscedasticity is to use weighted least squares regression where the parameters are fitted to a single observation based on its residual to correct for variation in the residuals (the scatter you can see above). But that is beyond the scope of this module.